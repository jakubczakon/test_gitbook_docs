<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>neptunecontrib.monitoring.xgboost API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>neptunecontrib.monitoring.xgboost</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#
# Copyright (c) 2020, Neptune Labs Sp. z o.o.
#
# Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import os
import tempfile

import neptune
import xgboost as xgb


def neptune_callback(log_model=True,
                     log_importance=True,
                     max_num_features=None,
                     log_tree=(0,),
                     experiment=None,
                     **kwargs):
    &#34;&#34;&#34;XGBoost callback for Neptune experiments.

    This is XGBoost callback that automatically logs training and evaluation metrics, feature importance chart,
    visualized trees and trained Booster to Neptune.

    Check Neptune documentation for the `full example &lt;https://docs.neptune.ai/integrations/xgboost.html&gt;`_.

    Make sure you created an experiment before you start XGBoost training using ``neptune.create_experiment()``
    (`check our docs &lt;https://docs.neptune.ai/neptune-client/docs/project.html
    #neptune.projects.Project.create_experiment&gt;`_).

    Integration works with ``xgboost&gt;=0.82``.

    Tip:
        Use this `Google Colab &lt;https://colab.research.google.com/github/neptune-ai/neptune-colab-examples
        /blob/master/xgboost-integration.ipynb&gt;`_ to try it without further ado.

    Args:
        log_model (:obj:`bool`, optional, default is ``True``):
            | Log booster to Neptune after last boosting iteration.
            | If you run xgb.cv, log booster for all folds.
        log_importance (:obj:`bool`, optional, default is ``True``):
            | Log feature importance to Neptune as image after last boosting iteration.
            | Specify number of features using ``max_num_features`` parameter below.
            | If you run xgb.cv, log feature importance for each folds&#39; booster.
        max_num_features (:obj:`int`, optional, default is ``None``):
            | Plot top ``max_num_features`` features on the importance plot.
            | If ``None``, plot all features.
        log_tree (:obj:`list` of :obj:`int`, optional, default is ``[1,]``):
            | Log specified trees to Neptune as images after last boosting iteration.
            | If you run xgb.cv, log specified trees for each folds&#39; booster.
            | Default is to log first tree.
            | If ``None``, do not log any tree.
        experiment (:obj:`neptune.experiments.Experiment`, optional, default is ``None``):
            | For advanced users only. Pass Neptune
              `Experiment &lt;https://docs.neptune.ai/neptune-client/docs/experiment.html#neptune.experiments.Experiment&gt;`_
              object if you want to control to which experiment data is logged.
            | If ``None``, log to currently active, and most recent experiment.
        kwargs:
            Parametrize XGBoost functions used in this callback:
            `xgboost.plot_importance &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html
            ?highlight=plot_tree#xgboost.plot_importance&gt;`_
            and `xgboost.to_graphviz &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html
            ?highlight=plot_tree#xgboost.to_graphviz&gt;`_.

    Returns:
        :obj:`callback`, function that you can pass directly to the XGBoost callbacks list, for example to the
        ``xgboost.cv()``
        (`see docs &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=plot_tree#xgboost.cv&gt;`_)
        or ``XGBClassifier.fit()``
        (`check docs &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=plot_tree
        #xgboost.XGBClassifier.fit&gt;`_).

    Note:
        If you use early stopping, make sure to log model, feature importance and trees on your own.
        Neptune logs these artifacts only after last iteration, which you may not reach because of early stop.

    Examples:
        ``xgb.train`` examples

        .. code:: python3

            # basic usage
            xgb.train(param, dtrain, num_round, watchlist,
                      callbacks=[neptune_callback()])

            # do not log model
            xgb.train(param, dtrain, num_round, watchlist,
                      callbacks=[neptune_callback(log_model=False)])

            # log top 5 features&#39; importance chart
            xgb.train(param, dtrain, num_round, watchlist,
                      callbacks=[neptune_callback(max_num_features=5)])

        ``xgb.cv`` examples

        .. code:: python3

            # log 5 trees per each folds&#39; booster
            xgb.cv(param, dtrain, num_boost_round=num_round, nfold=7,
                   callbacks=neptune_callback(log_tree=[0,1,2,3,4]))

            # log only metrics
            xgb.cv(param, dtrain, num_boost_round=num_round, nfold=7,
                   callbacks=[neptune_callback(log_model=False,
                                               log_importance=False,
                                               max_num_features=None,
                                               log_tree=None)])

            # log top 5 features per each folds&#39; booster
            xgb.cv(param, dtrain, num_boost_round=num_round, nfold=7,
                   callbacks=[neptune_callback(log_model=False,
                                               max_num_features=3,
                                               log_tree=None)])

        ``sklearn`` API examples

        .. code:: python3

            # basic usage with early stopping
            xgb.XGBRegressor().fit(X_train, y_train,
                                   early_stopping_rounds=10,
                                   eval_metric=[&#39;mae&#39;, &#39;rmse&#39;, &#39;rmsle&#39;],
                                   eval_set=[(X_test, y_test)],
                                   callbacks=[neptune_callback()])

            # do not log model
            clf = xgb.XGBRegressor()
            clf.fit(X_train, y_train,
                    eval_metric=[&#39;mae&#39;, &#39;rmse&#39;, &#39;rmsle&#39;],
                    eval_set=[(X_test, y_test)],
                    callbacks=[neptune_callback(log_model=False)])
            y_pred = clf.predict(X_test)

            # log 8 trees
            reg = xgb.XGBRegressor(**params)
            reg.fit(X_train, y_train,
                    eval_metric=[&#39;mae&#39;, &#39;rmse&#39;, &#39;rmsle&#39;],
                    eval_set=[(X_test, y_test)],
                    callbacks=[neptune_callback(log_tree=[0,1,2,3,4,5,6,7])])
    &#34;&#34;&#34;
    if experiment:
        _exp = experiment
    else:
        try:
            neptune.get_experiment()
            _exp = neptune
        except neptune.exceptions.NoExperimentContext:
            msg = &#39;No currently running Neptune experiment. \n&#39;\
                  &#39;To start logging to Neptune create experiment by using: `neptune.create_experiment()`. \n&#39;\
                  &#39;More info in the documentation: &#39;\
                  &#39;&lt;https://docs.neptune.ai/neptune-client/docs/project.html&#39; \
                  &#39;#neptune.projects.Project.create_experiment&gt;.&#39;
            raise neptune.exceptions.NeptuneException(msg)

    assert isinstance(log_model, bool),\
        &#39;log_model must be bool, got {} instead. Check log_model parameter.&#39;.format(type(log_model))
    assert isinstance(log_importance, bool),\
        &#39;log_importance must be bool, got {} instead. Check log_importance parameter.&#39;.format(type(log_importance))
    if max_num_features is not None:
        assert isinstance(max_num_features, int),\
            &#39;max_num_features must be int, got {} instead. &#39; \
            &#39;Check max_num_features parameter.&#39;.format(type(max_num_features))
    if log_tree is not None:
        if isinstance(log_tree, tuple):
            log_tree = list(log_tree)
        assert isinstance(log_tree, list),\
            &#39;log_tree must be list of int, got {} instead. Check log_tree parameter.&#39;.format(type(log_tree))

    def callback(env):
        # Log metrics after iteration
        for item in env.evaluation_result_list:
            if len(item) == 2:  # train case
                _exp.log_metric(item[0], item[1])
            if len(item) == 3:  # cv case
                _exp.log_metric(&#39;{}-mean&#39;.format(item[0]), item[1])
                _exp.log_metric(&#39;{}-std&#39;.format(item[0]), item[2])

        # Log booster, end of training
        if env.iteration + 1 == env.end_iteration and log_model:
            if env.cvfolds:  # cv case
                for i, cvpack in enumerate(env.cvfolds):
                    _log_model(cvpack.bst, &#39;cv-fold-{}-bst.model&#39;.format(i), _exp)
            else:  # train case
                _log_model(env.model, &#39;bst.model&#39;, _exp)

        # Log feature importance, end of training
        if env.iteration + 1 == env.end_iteration and log_importance:
            if env.cvfolds:  # cv case
                for i, cvpack in enumerate(env.cvfolds):
                    _log_importance(cvpack.bst, max_num_features, _exp, title=&#39;cv-fold-{}&#39;.format(i), **kwargs)
            else:  # train case
                _log_importance(env.model, max_num_features, _exp, **kwargs)

        # Log trees, end of training
        if env.iteration + 1 == env.end_iteration and log_tree:
            if env.cvfolds:
                for j, cvpack in enumerate(env.cvfolds):
                    _log_trees(cvpack.bst, log_tree, &#39;trees-cv-fold-{}&#39;.format(j), _exp, **kwargs)
            else:
                _log_trees(env.model, log_tree, &#39;trees&#39;, _exp, **kwargs)
    return callback


def _log_model(booster, name, npt):
    with tempfile.TemporaryDirectory(dir=&#39;.&#39;) as d:
        path = os.path.join(d, name)
        booster.save_model(path)
        npt.log_artifact(path)


def _log_importance(booster, max_num_features, npt, **kwargs):
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        raise ImportError(&#39;Please install matplotlib to log importance&#39;)
    importance = xgb.plot_importance(booster, max_num_features=max_num_features, **kwargs) # pylint: disable=E1101
    npt.log_image(&#39;feature_importance&#39;, importance.figure)
    plt.close(&#39;all&#39;)


def _log_trees(booster, tree_list, img_name, npt, **kwargs):
    with tempfile.TemporaryDirectory(dir=&#39;.&#39;) as d:
        for i in tree_list:
            file_name = &#39;tree_{}&#39;.format(i)
            tree = xgb.to_graphviz(booster=booster, num_trees=i, **kwargs) # pylint: disable=E1101
            tree.render(filename=file_name, directory=d, view=False, format=&#39;png&#39;)
            npt.log_image(img_name,
                          os.path.join(d, &#39;{}.png&#39;.format(file_name)),
                          image_name=file_name)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="neptunecontrib.monitoring.xgboost.neptune_callback"><code class="name flex">
<span>def <span class="ident">neptune_callback</span></span>(<span>log_model=True, log_importance=True, max_num_features=None, log_tree=(0,), experiment=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>XGBoost callback for Neptune experiments.</p>
<p>This is XGBoost callback that automatically logs training and evaluation metrics, feature importance chart,
visualized trees and trained Booster to Neptune.</p>
<p>Check Neptune documentation for the <code>full example &lt;https://docs.neptune.ai/integrations/xgboost.html&gt;</code>_.</p>
<p>Make sure you created an experiment before you start XGBoost training using <code>neptune.create_experiment()</code>
(`check our docs &lt;https://docs.neptune.ai/neptune-client/docs/project.html</p>
<h1 id="neptuneprojectsprojectcreate_experiment_">neptune.projects.Project.create_experiment&gt;`_).</h1>
<p>Integration works with <code>xgboost&gt;=0.82</code>.</p>
<h2 id="tip">Tip</h2>
<p>Use this <code>Google Colab &lt;https://colab.research.google.com/github/neptune-ai/neptune-colab-examples
/blob/master/xgboost-integration.ipynb&gt;</code>_ to try it without further ado.</p>
<h2 id="args">Args</h2>
<p>log_model (:obj:<code>bool</code>, optional, default is <code>True</code>):
| Log booster to Neptune after last boosting iteration.
| If you run xgb.cv, log booster for all folds.
log_importance (:obj:<code>bool</code>, optional, default is <code>True</code>):
| Log feature importance to Neptune as image after last boosting iteration.
| Specify number of features using <code>max_num_features</code> parameter below.
| If you run xgb.cv, log feature importance for each folds' booster.
max_num_features (:obj:<code>int</code>, optional, default is <code>None</code>):
| Plot top <code>max_num_features</code> features on the importance plot.
| If <code>None</code>, plot all features.
log_tree (:obj:<code>list</code> of :obj:<code>int</code>, optional, default is <code>[1,]</code>):
| Log specified trees to Neptune as images after last boosting iteration.
| If you run xgb.cv, log specified trees for each folds' booster.
| Default is to log first tree.
| If <code>None</code>, do not log any tree.
experiment (:obj:<code>neptune.experiments.Experiment</code>, optional, default is <code>None</code>):
| For advanced users only. Pass Neptune
<code>Experiment &lt;https://docs.neptune.ai/neptune-client/docs/experiment.html#neptune.experiments.Experiment&gt;</code><em>
object if you want to control to which experiment data is logged.
| If <code>None</code>, log to currently active, and most recent experiment.
kwargs:
Parametrize XGBoost functions used in this callback:
<code>xgboost.plot_importance &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html
?highlight=plot_tree#xgboost.plot_importance&gt;</code></em>
and <code>xgboost.to_graphviz &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html
?highlight=plot_tree#xgboost.to_graphviz&gt;</code>_.</p>
<h2 id="returns">Returns</h2>
<p>:obj:<code>callback</code>, function that you can pass directly to the XGBoost callbacks list, for example to the
<code>xgboost.cv()</code>
(<code>see docs &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=plot_tree#xgboost.cv&gt;</code>_)
or <code>XGBClassifier.fit()</code>
(`check docs &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=plot_tree</p>
<h1 id="xgboostxgbclassifierfit_">xgboost.XGBClassifier.fit&gt;`_).</h1>
<h2 id="note">Note</h2>
<p>If you use early stopping, make sure to log model, feature importance and trees on your own.
Neptune logs these artifacts only after last iteration, which you may not reach because of early stop.</p>
<h2 id="examples">Examples</h2>
<p><code>xgb.train</code> examples</p>
<div class="admonition code">
<p class="admonition-title">Code:&ensp;python3</p>
<h1 id="basic-usage">basic usage</h1>
<p>xgb.train(param, dtrain, num_round, watchlist,
callbacks=[neptune_callback()])</p>
<h1 id="do-not-log-model">do not log model</h1>
<p>xgb.train(param, dtrain, num_round, watchlist,
callbacks=[neptune_callback(log_model=False)])</p>
<h1 id="log-top-5-features-importance-chart">log top 5 features' importance chart</h1>
<p>xgb.train(param, dtrain, num_round, watchlist,
callbacks=[neptune_callback(max_num_features=5)])</p>
</div>
<p><code>xgb.cv</code> examples</p>
<div class="admonition code">
<p class="admonition-title">Code:&ensp;python3</p>
<h1 id="log-5-trees-per-each-folds-booster">log 5 trees per each folds' booster</h1>
<p>xgb.cv(param, dtrain, num_boost_round=num_round, nfold=7,
callbacks=neptune_callback(log_tree=[0,1,2,3,4]))</p>
<h1 id="log-only-metrics">log only metrics</h1>
<p>xgb.cv(param, dtrain, num_boost_round=num_round, nfold=7,
callbacks=[neptune_callback(log_model=False,
log_importance=False,
max_num_features=None,
log_tree=None)])</p>
<h1 id="log-top-5-features-per-each-folds-booster">log top 5 features per each folds' booster</h1>
<p>xgb.cv(param, dtrain, num_boost_round=num_round, nfold=7,
callbacks=[neptune_callback(log_model=False,
max_num_features=3,
log_tree=None)])</p>
</div>
<p><code>sklearn</code> API examples</p>
<div class="admonition code">
<p class="admonition-title">Code:&ensp;python3</p>
<h1 id="basic-usage-with-early-stopping">basic usage with early stopping</h1>
<p>xgb.XGBRegressor().fit(X_train, y_train,
early_stopping_rounds=10,
eval_metric=['mae', 'rmse', 'rmsle'],
eval_set=[(X_test, y_test)],
callbacks=[neptune_callback()])</p>
<h1 id="do-not-log-model_1">do not log model</h1>
<p>clf = xgb.XGBRegressor()
clf.fit(X_train, y_train,
eval_metric=['mae', 'rmse', 'rmsle'],
eval_set=[(X_test, y_test)],
callbacks=[neptune_callback(log_model=False)])
y_pred = clf.predict(X_test)</p>
<h1 id="log-8-trees">log 8 trees</h1>
<p>reg = xgb.XGBRegressor(**params)
reg.fit(X_train, y_train,
eval_metric=['mae', 'rmse', 'rmsle'],
eval_set=[(X_test, y_test)],
callbacks=[neptune_callback(log_tree=[0,1,2,3,4,5,6,7])])</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def neptune_callback(log_model=True,
                     log_importance=True,
                     max_num_features=None,
                     log_tree=(0,),
                     experiment=None,
                     **kwargs):
    &#34;&#34;&#34;XGBoost callback for Neptune experiments.

    This is XGBoost callback that automatically logs training and evaluation metrics, feature importance chart,
    visualized trees and trained Booster to Neptune.

    Check Neptune documentation for the `full example &lt;https://docs.neptune.ai/integrations/xgboost.html&gt;`_.

    Make sure you created an experiment before you start XGBoost training using ``neptune.create_experiment()``
    (`check our docs &lt;https://docs.neptune.ai/neptune-client/docs/project.html
    #neptune.projects.Project.create_experiment&gt;`_).

    Integration works with ``xgboost&gt;=0.82``.

    Tip:
        Use this `Google Colab &lt;https://colab.research.google.com/github/neptune-ai/neptune-colab-examples
        /blob/master/xgboost-integration.ipynb&gt;`_ to try it without further ado.

    Args:
        log_model (:obj:`bool`, optional, default is ``True``):
            | Log booster to Neptune after last boosting iteration.
            | If you run xgb.cv, log booster for all folds.
        log_importance (:obj:`bool`, optional, default is ``True``):
            | Log feature importance to Neptune as image after last boosting iteration.
            | Specify number of features using ``max_num_features`` parameter below.
            | If you run xgb.cv, log feature importance for each folds&#39; booster.
        max_num_features (:obj:`int`, optional, default is ``None``):
            | Plot top ``max_num_features`` features on the importance plot.
            | If ``None``, plot all features.
        log_tree (:obj:`list` of :obj:`int`, optional, default is ``[1,]``):
            | Log specified trees to Neptune as images after last boosting iteration.
            | If you run xgb.cv, log specified trees for each folds&#39; booster.
            | Default is to log first tree.
            | If ``None``, do not log any tree.
        experiment (:obj:`neptune.experiments.Experiment`, optional, default is ``None``):
            | For advanced users only. Pass Neptune
              `Experiment &lt;https://docs.neptune.ai/neptune-client/docs/experiment.html#neptune.experiments.Experiment&gt;`_
              object if you want to control to which experiment data is logged.
            | If ``None``, log to currently active, and most recent experiment.
        kwargs:
            Parametrize XGBoost functions used in this callback:
            `xgboost.plot_importance &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html
            ?highlight=plot_tree#xgboost.plot_importance&gt;`_
            and `xgboost.to_graphviz &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html
            ?highlight=plot_tree#xgboost.to_graphviz&gt;`_.

    Returns:
        :obj:`callback`, function that you can pass directly to the XGBoost callbacks list, for example to the
        ``xgboost.cv()``
        (`see docs &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=plot_tree#xgboost.cv&gt;`_)
        or ``XGBClassifier.fit()``
        (`check docs &lt;https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=plot_tree
        #xgboost.XGBClassifier.fit&gt;`_).

    Note:
        If you use early stopping, make sure to log model, feature importance and trees on your own.
        Neptune logs these artifacts only after last iteration, which you may not reach because of early stop.

    Examples:
        ``xgb.train`` examples

        .. code:: python3

            # basic usage
            xgb.train(param, dtrain, num_round, watchlist,
                      callbacks=[neptune_callback()])

            # do not log model
            xgb.train(param, dtrain, num_round, watchlist,
                      callbacks=[neptune_callback(log_model=False)])

            # log top 5 features&#39; importance chart
            xgb.train(param, dtrain, num_round, watchlist,
                      callbacks=[neptune_callback(max_num_features=5)])

        ``xgb.cv`` examples

        .. code:: python3

            # log 5 trees per each folds&#39; booster
            xgb.cv(param, dtrain, num_boost_round=num_round, nfold=7,
                   callbacks=neptune_callback(log_tree=[0,1,2,3,4]))

            # log only metrics
            xgb.cv(param, dtrain, num_boost_round=num_round, nfold=7,
                   callbacks=[neptune_callback(log_model=False,
                                               log_importance=False,
                                               max_num_features=None,
                                               log_tree=None)])

            # log top 5 features per each folds&#39; booster
            xgb.cv(param, dtrain, num_boost_round=num_round, nfold=7,
                   callbacks=[neptune_callback(log_model=False,
                                               max_num_features=3,
                                               log_tree=None)])

        ``sklearn`` API examples

        .. code:: python3

            # basic usage with early stopping
            xgb.XGBRegressor().fit(X_train, y_train,
                                   early_stopping_rounds=10,
                                   eval_metric=[&#39;mae&#39;, &#39;rmse&#39;, &#39;rmsle&#39;],
                                   eval_set=[(X_test, y_test)],
                                   callbacks=[neptune_callback()])

            # do not log model
            clf = xgb.XGBRegressor()
            clf.fit(X_train, y_train,
                    eval_metric=[&#39;mae&#39;, &#39;rmse&#39;, &#39;rmsle&#39;],
                    eval_set=[(X_test, y_test)],
                    callbacks=[neptune_callback(log_model=False)])
            y_pred = clf.predict(X_test)

            # log 8 trees
            reg = xgb.XGBRegressor(**params)
            reg.fit(X_train, y_train,
                    eval_metric=[&#39;mae&#39;, &#39;rmse&#39;, &#39;rmsle&#39;],
                    eval_set=[(X_test, y_test)],
                    callbacks=[neptune_callback(log_tree=[0,1,2,3,4,5,6,7])])
    &#34;&#34;&#34;
    if experiment:
        _exp = experiment
    else:
        try:
            neptune.get_experiment()
            _exp = neptune
        except neptune.exceptions.NoExperimentContext:
            msg = &#39;No currently running Neptune experiment. \n&#39;\
                  &#39;To start logging to Neptune create experiment by using: `neptune.create_experiment()`. \n&#39;\
                  &#39;More info in the documentation: &#39;\
                  &#39;&lt;https://docs.neptune.ai/neptune-client/docs/project.html&#39; \
                  &#39;#neptune.projects.Project.create_experiment&gt;.&#39;
            raise neptune.exceptions.NeptuneException(msg)

    assert isinstance(log_model, bool),\
        &#39;log_model must be bool, got {} instead. Check log_model parameter.&#39;.format(type(log_model))
    assert isinstance(log_importance, bool),\
        &#39;log_importance must be bool, got {} instead. Check log_importance parameter.&#39;.format(type(log_importance))
    if max_num_features is not None:
        assert isinstance(max_num_features, int),\
            &#39;max_num_features must be int, got {} instead. &#39; \
            &#39;Check max_num_features parameter.&#39;.format(type(max_num_features))
    if log_tree is not None:
        if isinstance(log_tree, tuple):
            log_tree = list(log_tree)
        assert isinstance(log_tree, list),\
            &#39;log_tree must be list of int, got {} instead. Check log_tree parameter.&#39;.format(type(log_tree))

    def callback(env):
        # Log metrics after iteration
        for item in env.evaluation_result_list:
            if len(item) == 2:  # train case
                _exp.log_metric(item[0], item[1])
            if len(item) == 3:  # cv case
                _exp.log_metric(&#39;{}-mean&#39;.format(item[0]), item[1])
                _exp.log_metric(&#39;{}-std&#39;.format(item[0]), item[2])

        # Log booster, end of training
        if env.iteration + 1 == env.end_iteration and log_model:
            if env.cvfolds:  # cv case
                for i, cvpack in enumerate(env.cvfolds):
                    _log_model(cvpack.bst, &#39;cv-fold-{}-bst.model&#39;.format(i), _exp)
            else:  # train case
                _log_model(env.model, &#39;bst.model&#39;, _exp)

        # Log feature importance, end of training
        if env.iteration + 1 == env.end_iteration and log_importance:
            if env.cvfolds:  # cv case
                for i, cvpack in enumerate(env.cvfolds):
                    _log_importance(cvpack.bst, max_num_features, _exp, title=&#39;cv-fold-{}&#39;.format(i), **kwargs)
            else:  # train case
                _log_importance(env.model, max_num_features, _exp, **kwargs)

        # Log trees, end of training
        if env.iteration + 1 == env.end_iteration and log_tree:
            if env.cvfolds:
                for j, cvpack in enumerate(env.cvfolds):
                    _log_trees(cvpack.bst, log_tree, &#39;trees-cv-fold-{}&#39;.format(j), _exp, **kwargs)
            else:
                _log_trees(env.model, log_tree, &#39;trees&#39;, _exp, **kwargs)
    return callback</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="neptunecontrib.monitoring" href="index.html">neptunecontrib.monitoring</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="neptunecontrib.monitoring.xgboost.neptune_callback" href="#neptunecontrib.monitoring.xgboost.neptune_callback">neptune_callback</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>