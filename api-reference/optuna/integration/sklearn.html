<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>optuna.integration.sklearn API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>optuna.integration.sklearn</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from logging import DEBUG
from logging import INFO
from logging import WARNING
from numbers import Integral
from numbers import Number
from time import time
from typing import Any
from typing import Callable
from typing import Dict
from typing import Iterable
from typing import List
from typing import Mapping
from typing import Optional
from typing import Union

import numpy as np
import scipy as sp
from scipy.sparse import spmatrix

from optuna._experimental import experimental
from optuna._imports import try_import
from optuna import distributions
from optuna import logging
from optuna import samplers
from optuna import study as study_module
from optuna.study import StudyDirection
from optuna.trial import FrozenTrial
from optuna.trial import Trial
from optuna import TrialPruned

with try_import() as _imports:
    import pandas as pd
    import sklearn
    from sklearn.base import BaseEstimator
    from sklearn.base import clone
    from sklearn.base import is_classifier
    from sklearn.metrics.scorer import check_scoring
    from sklearn.model_selection import BaseCrossValidator
    from sklearn.model_selection import check_cv
    from sklearn.model_selection import cross_validate
    from sklearn.utils import check_random_state
    from sklearn.utils.metaestimators import _safe_split

    if sklearn.__version__ &gt;= &#34;0.22&#34;:
        from sklearn.utils import _safe_indexing as sklearn_safe_indexing
    else:
        from sklearn.utils import safe_indexing as sklearn_safe_indexing
    from sklearn.utils.validation import check_is_fitted

if not _imports.is_successful():
    BaseEstimator = object  # NOQA

ArrayLikeType = Union[List, np.ndarray, &#34;pd.Series&#34;, spmatrix]
OneDimArrayLikeType = Union[List[float], np.ndarray, &#34;pd.Series&#34;]
TwoDimArrayLikeType = Union[List[List[float]], np.ndarray, &#34;pd.DataFrame&#34;, spmatrix]
IterableType = Union[List, &#34;pd.DataFrame&#34;, np.ndarray, &#34;pd.Series&#34;, spmatrix, None]
IndexableType = Union[Iterable, None]

_logger = logging.get_logger(__name__)


def _check_fit_params(
    X: TwoDimArrayLikeType, fit_params: Dict, indices: OneDimArrayLikeType
) -&gt; Dict:

    fit_params_validated = {}
    for key, value in fit_params.items():

        # NOTE Original implementation:
        # https://github.com/scikit-learn/scikit-learn/blob/ \
        # 2467e1b84aeb493a22533fa15ff92e0d7c05ed1c/sklearn/utils/validation.py#L1324-L1328
        # Scikit-learn does not accept non-iterable inputs.
        # This line is for keeping backward compatibility.
        # (See: https://github.com/scikit-learn/scikit-learn/issues/15805)
        if not _is_arraylike(value) or _num_samples(value) != _num_samples(X):
            fit_params_validated[key] = value
        else:
            fit_params_validated[key] = _make_indexable(value)
            fit_params_validated[key] = _safe_indexing(fit_params_validated[key], indices)
    return fit_params_validated


# NOTE Original implementation:
# https://github.com/scikit-learn/scikit-learn/blob/ \
# 8caa93889f85254fc3ca84caa0a24a1640eebdd1/sklearn/utils/validation.py#L131-L135
def _is_arraylike(x: Any) -&gt; bool:

    return hasattr(x, &#34;__len__&#34;) or hasattr(x, &#34;shape&#34;) or hasattr(x, &#34;__array__&#34;)


# NOTE Original implementation:
# https://github.com/scikit-learn/scikit-learn/blob/ \
# 8caa93889f85254fc3ca84caa0a24a1640eebdd1/sklearn/utils/validation.py#L217-L234
def _make_indexable(iterable: IterableType) -&gt; IndexableType:

    tocsr_func = getattr(iterable, &#34;tocsr&#34;, None)
    if tocsr_func is not None and sp.sparse.issparse(iterable):
        return tocsr_func(iterable)
    elif hasattr(iterable, &#34;__getitem__&#34;) or hasattr(iterable, &#34;iloc&#34;):
        return iterable
    elif iterable is None:
        return iterable
    return np.array(iterable)


def _num_samples(x: ArrayLikeType) -&gt; int:

    # NOTE For dask dataframes
    # https://github.com/scikit-learn/scikit-learn/blob/ \
    # 8caa93889f85254fc3ca84caa0a24a1640eebdd1/sklearn/utils/validation.py#L155-L158
    x_shape = getattr(x, &#34;shape&#34;, None)
    if x_shape is not None:
        if isinstance(x_shape[0], Integral):
            return int(x_shape[0])

    try:
        return len(x)
    except TypeError:
        raise TypeError(&#34;Expected sequence or array-like, got %s.&#34; % type(x))


def _safe_indexing(
    X: Union[OneDimArrayLikeType, TwoDimArrayLikeType], indices: OneDimArrayLikeType
) -&gt; Union[OneDimArrayLikeType, TwoDimArrayLikeType]:

    if X is None:
        return X

    return sklearn_safe_indexing(X, indices)


class _Objective(object):
    &#34;&#34;&#34;Callable that implements objective function.

    Args:
        estimator:
            Object to use to fit the data. This is assumed to implement the
            scikit-learn estimator interface. Either this needs to provide
            ``score``, or ``scoring`` must be passed.

        param_distributions:
            Dictionary where keys are parameters and values are distributions.
            Distributions are assumed to implement the optuna distribution
            interface.

        X:
            Training data.

        y:
            Target variable.

        cv:
            Cross-validation strategy.

        enable_pruning:
            If :obj:`True`, pruning is performed in the case where the
            underlying estimator supports ``partial_fit``.

        error_score:
            Value to assign to the score if an error occurs in fitting. If
            &#39;raise&#39;, the error is raised. If numeric,
            ``sklearn.exceptions.FitFailedWarning`` is raised. This does not
            affect the refit step, which will always raise the error.

        fit_params:
            Parameters passed to ``fit`` one the estimator.

        groups:
            Group labels for the samples used while splitting the dataset into
            train/validation set.

        max_iter:
            Maximum number of epochs. This is only used if the underlying
            estimator supports ``partial_fit``.

        return_train_score:
            If :obj:`True`, training scores will be included. Computing
            training scores is used to get insights on how different
            hyperparameter settings impact the overfitting/underfitting
            trade-off. However computing training scores can be
            computationally expensive and is not strictly required to select
            the hyperparameters that yield the best generalization
            performance.

        scoring:
            Scorer function.
    &#34;&#34;&#34;

    def __init__(
        self,
        estimator: &#34;BaseEstimator&#34;,
        param_distributions: Mapping[str, distributions.BaseDistribution],
        X: TwoDimArrayLikeType,
        y: Optional[Union[OneDimArrayLikeType, TwoDimArrayLikeType]],
        cv: &#34;BaseCrossValidator&#34;,
        enable_pruning: bool,
        error_score: Union[Number, str],
        fit_params: Dict[str, Any],
        groups: Optional[OneDimArrayLikeType],
        max_iter: int,
        return_train_score: bool,
        scoring: Callable[..., Number],
    ) -&gt; None:

        self.cv = cv
        self.enable_pruning = enable_pruning
        self.error_score = error_score
        self.estimator = estimator
        self.fit_params = fit_params
        self.groups = groups
        self.max_iter = max_iter
        self.param_distributions = param_distributions
        self.return_train_score = return_train_score
        self.scoring = scoring
        self.X = X
        self.y = y

    def __call__(self, trial: Trial) -&gt; float:

        estimator = clone(self.estimator)
        params = self._get_params(trial)

        estimator.set_params(**params)

        if self.enable_pruning:
            scores = self._cross_validate_with_pruning(trial, estimator)
        else:
            scores = cross_validate(
                estimator,
                self.X,
                self.y,
                cv=self.cv,
                error_score=self.error_score,
                fit_params=self.fit_params,
                groups=self.groups,
                return_train_score=self.return_train_score,
                scoring=self.scoring,
            )

        self._store_scores(trial, scores)

        return trial.user_attrs[&#34;mean_test_score&#34;]

    def _cross_validate_with_pruning(
        self, trial: Trial, estimator: &#34;BaseEstimator&#34;
    ) -&gt; Dict[str, OneDimArrayLikeType]:

        if is_classifier(estimator):
            partial_fit_params = self.fit_params.copy()
            classes = np.unique(self.y)

            partial_fit_params.setdefault(&#34;classes&#34;, classes)

        else:
            partial_fit_params = self.fit_params

        n_splits = self.cv.get_n_splits(self.X, self.y, groups=self.groups)
        estimators = [clone(estimator) for _ in range(n_splits)]
        scores = {
            &#34;fit_time&#34;: np.zeros(n_splits),
            &#34;score_time&#34;: np.zeros(n_splits),
            &#34;test_score&#34;: np.empty(n_splits),
        }

        if self.return_train_score:
            scores[&#34;train_score&#34;] = np.empty(n_splits)

        for step in range(self.max_iter):
            for i, (train, test) in enumerate(self.cv.split(self.X, self.y, groups=self.groups)):
                out = self._partial_fit_and_score(estimators[i], train, test, partial_fit_params)

                if self.return_train_score:
                    scores[&#34;train_score&#34;][i] = out.pop(0)

                scores[&#34;test_score&#34;][i] = out[0]
                scores[&#34;fit_time&#34;][i] += out[1]
                scores[&#34;score_time&#34;][i] += out[2]

            intermediate_value = np.nanmean(scores[&#34;test_score&#34;])

            trial.report(intermediate_value, step=step)

            if trial.should_prune():
                self._store_scores(trial, scores)

                raise TrialPruned(&#34;trial was pruned at iteration {}.&#34;.format(step))

        return scores

    def _get_params(self, trial: Trial) -&gt; Dict[str, Any]:

        return {
            name: trial._suggest(name, distribution)
            for name, distribution in self.param_distributions.items()
        }

    def _partial_fit_and_score(
        self,
        estimator: &#34;BaseEstimator&#34;,
        train: List[int],
        test: List[int],
        partial_fit_params: Dict[str, Any],
    ) -&gt; List[Number]:

        X_train, y_train = _safe_split(estimator, self.X, self.y, train)
        X_test, y_test = _safe_split(estimator, self.X, self.y, test, train_indices=train)

        start_time = time()

        try:
            estimator.partial_fit(X_train, y_train, **partial_fit_params)

        except Exception as e:
            if self.error_score == &#34;raise&#34;:
                raise e

            elif isinstance(self.error_score, Number):
                fit_time = time() - start_time
                test_score = self.error_score
                score_time = 0.0

                if self.return_train_score:
                    train_score = self.error_score

            else:
                raise ValueError(&#34;error_score must be &#39;raise&#39; or numeric.&#34;)

        else:
            fit_time = time() - start_time
            test_score = self.scoring(estimator, X_test, y_test)
            score_time = time() - fit_time - start_time

            if self.return_train_score:
                train_score = self.scoring(estimator, X_train, y_train)

        # Required for type checking but is never expected to fail.
        assert isinstance(fit_time, Number)
        assert isinstance(score_time, Number)

        ret = [test_score, fit_time, score_time]

        if self.return_train_score:
            ret.insert(0, train_score)

        return ret

    def _store_scores(self, trial: Trial, scores: Dict[str, OneDimArrayLikeType]) -&gt; None:

        for name, array in scores.items():
            if name in [&#34;test_score&#34;, &#34;train_score&#34;]:
                for i, score in enumerate(array):
                    trial.set_user_attr(&#34;split{}_{}&#34;.format(i, name), score)

            trial.set_user_attr(&#34;mean_{}&#34;.format(name), np.nanmean(array))
            trial.set_user_attr(&#34;std_{}&#34;.format(name), np.nanstd(array))


@experimental(&#34;0.17.0&#34;)
class OptunaSearchCV(BaseEstimator):
    &#34;&#34;&#34;Hyperparameter search with cross-validation.

    Args:
        estimator:
            Object to use to fit the data. This is assumed to implement the
            scikit-learn estimator interface. Either this needs to provide
            ``score``, or ``scoring`` must be passed.

        param_distributions:
            Dictionary where keys are parameters and values are distributions.
            Distributions are assumed to implement the optuna distribution
            interface.

        cv:
            Cross-validation strategy. Possible inputs for cv are:

            - integer to specify the number of folds in a CV splitter,
            - a CV splitter,
            - an iterable yielding (train, validation) splits as arrays of indices.

            For integer, if :obj:`estimator` is a classifier and :obj:`y` is
            either binary or multiclass,
            ``sklearn.model_selection.StratifiedKFold`` is used. otherwise,
            ``sklearn.model_selection.KFold`` is used.

        enable_pruning:
            If :obj:`True`, pruning is performed in the case where the
            underlying estimator supports ``partial_fit``.

        error_score:
            Value to assign to the score if an error occurs in fitting. If
            &#39;raise&#39;, the error is raised. If numeric,
            ``sklearn.exceptions.FitFailedWarning`` is raised. This does not
            affect the refit step, which will always raise the error.

        max_iter:
            Maximum number of epochs. This is only used if the underlying
            estimator supports ``partial_fit``.

        n_jobs:
            Number of parallel jobs. :obj:`-1` means using all processors.

        n_trials:
            Number of trials. If :obj:`None`, there is no limitation on the
            number of trials. If :obj:`timeout` is also set to :obj:`None`,
            the study continues to create trials until it receives a
            termination signal such as Ctrl+C or SIGTERM. This trades off
            runtime vs quality of the solution.

        random_state:
            Seed of the pseudo random number generator. If int, this is the
            seed used by the random number generator. If
            ``numpy.random.RandomState`` object, this is the random number
            generator. If :obj:`None`, the global random state from
            ``numpy.random`` is used.

        refit:
            If :obj:`True`, refit the estimator with the best found
            hyperparameters. The refitted estimator is made available at the
            ``best_estimator_`` attribute and permits using ``predict``
            directly.

        return_train_score:
            If :obj:`True`, training scores will be included. Computing
            training scores is used to get insights on how different
            hyperparameter settings impact the overfitting/underfitting
            trade-off. However computing training scores can be
            computationally expensive and is not strictly required to select
            the hyperparameters that yield the best generalization
            performance.

        scoring:
            String or callable to evaluate the predictions on the validation data.
            If :obj:`None`, ``score`` on the estimator is used.

        study:
            Study corresponds to the optimization task. If :obj:`None`, a new
            study is created.

        subsample:
            Proportion of samples that are used during hyperparameter search.

            - If int, then draw ``subsample`` samples.
            - If float, then draw ``subsample`` * ``X.shape[0]`` samples.

        timeout:
            Time limit in seconds for the search of appropriate models. If
            :obj:`None`, the study is executed without time limitation. If
            :obj:`n_trials` is also set to :obj:`None`, the study continues to
            create trials until it receives a termination signal such as
            Ctrl+C or SIGTERM. This trades off runtime vs quality of the
            solution.

        verbose:
            Verbosity level. The higher, the more messages.

    Attributes:
        best_estimator_:
            Estimator that was chosen by the search. This is present only if
            ``refit`` is set to :obj:`True`.

        n_splits_:
            Number of cross-validation splits.

        refit_time_:
            Time for refitting the best estimator. This is present only if
            ``refit`` is set to :obj:`True`.

        sample_indices_:
            Indices of samples that are used during hyperparameter search.

        scorer_:
            Scorer function.

        study_:
            Actual study.

    Examples:

        .. testcode::

                import optuna
                from sklearn.datasets import load_iris
                from sklearn.svm import SVC

                clf = SVC(gamma=&#39;auto&#39;)
                param_distributions = {
                    &#39;C&#39;: optuna.distributions.LogUniformDistribution(1e-10, 1e+10)
                }
                optuna_search = optuna.integration.OptunaSearchCV(
                    clf,
                    param_distributions
                )
                X, y = load_iris(return_X_y=True)
                optuna_search.fit(X, y)
                y_pred = optuna_search.predict(X)
    &#34;&#34;&#34;

    _required_parameters = [&#34;estimator&#34;, &#34;param_distributions&#34;]

    @property
    def _estimator_type(self) -&gt; str:

        return self.estimator._estimator_type

    @property
    def best_index_(self) -&gt; int:
        &#34;&#34;&#34;Index which corresponds to the best candidate parameter setting.&#34;&#34;&#34;

        df = self.trials_dataframe()

        return df[&#34;value&#34;].idxmin()

    @property
    def best_params_(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Parameters of the best trial in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.best_params

    @property
    def best_score_(self) -&gt; float:
        &#34;&#34;&#34;Mean cross-validated score of the best estimator.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.best_value

    @property
    def best_trial_(self) -&gt; FrozenTrial:
        &#34;&#34;&#34;Best trial in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.best_trial

    @property
    def classes_(self) -&gt; OneDimArrayLikeType:
        &#34;&#34;&#34;Class labels.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.classes_

    @property
    def n_trials_(self) -&gt; int:
        &#34;&#34;&#34;Actual number of trials.&#34;&#34;&#34;

        return len(self.trials_)

    @property
    def trials_(self) -&gt; List[FrozenTrial]:
        &#34;&#34;&#34;All trials in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.trials

    @property
    def user_attrs_(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;User attributes in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.user_attrs

    @property
    def decision_function(self) -&gt; Callable[..., Union[OneDimArrayLikeType, TwoDimArrayLikeType]]:
        &#34;&#34;&#34;Call ``decision_function`` on the best estimator.

        This is available only if the underlying estimator supports
        ``decision_function`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.decision_function

    @property
    def inverse_transform(self) -&gt; Callable[..., TwoDimArrayLikeType]:
        &#34;&#34;&#34;Call ``inverse_transform`` on the best estimator.

        This is available only if the underlying estimator supports
        ``inverse_transform`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.inverse_transform

    @property
    def predict(self) -&gt; Callable[..., Union[OneDimArrayLikeType, TwoDimArrayLikeType]]:
        &#34;&#34;&#34;Call ``predict`` on the best estimator.

        This is available only if the underlying estimator supports ``predict``
        and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.predict

    @property
    def predict_log_proba(self) -&gt; Callable[..., TwoDimArrayLikeType]:
        &#34;&#34;&#34;Call ``predict_log_proba`` on the best estimator.

        This is available only if the underlying estimator supports
        ``predict_log_proba`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.predict_log_proba

    @property
    def predict_proba(self) -&gt; Callable[..., TwoDimArrayLikeType]:
        &#34;&#34;&#34;Call ``predict_proba`` on the best estimator.

        This is available only if the underlying estimator supports
        ``predict_proba`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.predict_proba

    @property
    def score_samples(self) -&gt; Callable[..., OneDimArrayLikeType]:
        &#34;&#34;&#34;Call ``score_samples`` on the best estimator.

        This is available only if the underlying estimator supports
        ``score_samples`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.score_samples

    @property
    def set_user_attr(self) -&gt; Callable[..., None]:
        &#34;&#34;&#34;Call ``set_user_attr`` on the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.set_user_attr

    @property
    def transform(self) -&gt; Callable[..., TwoDimArrayLikeType]:
        &#34;&#34;&#34;Call ``transform`` on the best estimator.

        This is available only if the underlying estimator supports
        ``transform`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.transform

    @property
    def trials_dataframe(self) -&gt; Callable[..., &#34;pd.DataFrame&#34;]:
        &#34;&#34;&#34;Call ``trials_dataframe`` on the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.trials_dataframe

    def __init__(
        self,
        estimator: &#34;BaseEstimator&#34;,
        param_distributions: Mapping[str, distributions.BaseDistribution],
        cv: Optional[Union[&#34;BaseCrossValidator&#34;, int]] = 5,
        enable_pruning: bool = False,
        error_score: Union[Number, str] = np.nan,
        max_iter: int = 1000,
        n_jobs: int = 1,
        n_trials: int = 10,
        random_state: Optional[Union[int, np.random.RandomState]] = None,
        refit: bool = True,
        return_train_score: bool = False,
        scoring: Optional[Union[Callable[..., float], str]] = None,
        study: Optional[study_module.Study] = None,
        subsample: Union[float, int] = 1.0,
        timeout: Optional[float] = None,
        verbose: int = 0,
    ) -&gt; None:

        _imports.check()

        self.cv = cv
        self.enable_pruning = enable_pruning
        self.error_score = error_score
        self.estimator = estimator
        self.max_iter = max_iter
        self.n_trials = n_trials
        self.n_jobs = n_jobs
        self.param_distributions = param_distributions
        self.random_state = random_state
        self.refit = refit
        self.return_train_score = return_train_score
        self.scoring = scoring
        self.study = study
        self.subsample = subsample
        self.timeout = timeout
        self.verbose = verbose

    def _check_is_fitted(self) -&gt; None:

        attributes = [&#34;n_splits_&#34;, &#34;sample_indices_&#34;, &#34;scorer_&#34;, &#34;study_&#34;]

        if self.refit:
            attributes += [&#34;best_estimator_&#34;, &#34;refit_time_&#34;]

        check_is_fitted(self, attributes)

    def _check_params(self) -&gt; None:

        if not hasattr(self.estimator, &#34;fit&#34;):
            raise ValueError(&#34;estimator must be a scikit-learn estimator.&#34;)

        if type(self.param_distributions) is not dict:
            raise ValueError(&#34;param_distributions must be a dictionary.&#34;)

        for name, distribution in self.param_distributions.items():
            if not isinstance(distribution, distributions.BaseDistribution):
                raise ValueError(&#34;Value of {} must be a optuna distribution.&#34;.format(name))

        if self.enable_pruning and not hasattr(self.estimator, &#34;partial_fit&#34;):
            raise ValueError(&#34;estimator must support partial_fit.&#34;)

        if self.max_iter &lt;= 0:
            raise ValueError(&#34;max_iter must be &gt; 0, got {}.&#34;.format(self.max_iter))

        if self.study is not None and self.study.direction != StudyDirection.MAXIMIZE:
            raise ValueError(&#34;direction of study must be &#39;maximize&#39;.&#34;)

    def _more_tags(self) -&gt; Dict[str, bool]:

        return {&#34;non_deterministic&#34;: True, &#34;no_validation&#34;: True}

    def _refit(
        self,
        X: TwoDimArrayLikeType,
        y: Optional[Union[OneDimArrayLikeType, TwoDimArrayLikeType]] = None,
        **fit_params: Any
    ) -&gt; &#34;OptunaSearchCV&#34;:

        n_samples = _num_samples(X)

        self.best_estimator_ = clone(self.estimator)

        try:
            self.best_estimator_.set_params(**self.study_.best_params)
        except ValueError as e:
            _logger.exception(e)

        _logger.info(&#34;Refitting the estimator using {} samples...&#34;.format(n_samples))

        start_time = time()

        self.best_estimator_.fit(X, y, **fit_params)

        self.refit_time_ = time() - start_time

        _logger.info(&#34;Finished refitting! (elapsed time: {:.3f} sec.)&#34;.format(self.refit_time_))

        return self

    def fit(
        self,
        X: TwoDimArrayLikeType,
        y: Optional[Union[OneDimArrayLikeType, TwoDimArrayLikeType]] = None,
        groups: Optional[OneDimArrayLikeType] = None,
        **fit_params: Any
    ) -&gt; &#34;OptunaSearchCV&#34;:
        &#34;&#34;&#34;Run fit with all sets of parameters.

        Args:
            X:
                Training data.

            y:
                Target variable.

            groups:
                Group labels for the samples used while splitting the dataset
                into train/validation set.

            **fit_params:
                Parameters passed to ``fit`` on the estimator.

        Returns:
            self:
                Return self.
        &#34;&#34;&#34;

        self._check_params()

        random_state = check_random_state(self.random_state)
        max_samples = self.subsample
        n_samples = _num_samples(X)
        old_level = _logger.getEffectiveLevel()

        if self.verbose &gt; 1:
            _logger.setLevel(DEBUG)
        elif self.verbose &gt; 0:
            _logger.setLevel(INFO)
        else:
            _logger.setLevel(WARNING)

        self.sample_indices_ = np.arange(n_samples)

        if type(max_samples) is float:
            max_samples = int(max_samples * n_samples)

        if max_samples &lt; n_samples:
            self.sample_indices_ = random_state.choice(
                self.sample_indices_, max_samples, replace=False
            )

            self.sample_indices_.sort()

        X_res = _safe_indexing(X, self.sample_indices_)
        y_res = _safe_indexing(y, self.sample_indices_)
        groups_res = _safe_indexing(groups, self.sample_indices_)
        fit_params_res = fit_params

        if fit_params_res is not None:
            fit_params_res = _check_fit_params(X, fit_params, self.sample_indices_)

        classifier = is_classifier(self.estimator)
        cv = check_cv(self.cv, y_res, classifier)

        self.n_splits_ = cv.get_n_splits(X_res, y_res, groups=groups_res)
        self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)

        if self.study is None:
            seed = random_state.randint(0, np.iinfo(&#34;int32&#34;).max)
            sampler = samplers.TPESampler(seed=seed)

            self.study_ = study_module.create_study(direction=&#34;maximize&#34;, sampler=sampler)

        else:
            self.study_ = self.study

        objective = _Objective(
            self.estimator,
            self.param_distributions,
            X_res,
            y_res,
            cv,
            self.enable_pruning,
            self.error_score,
            fit_params_res,
            groups_res,
            self.max_iter,
            self.return_train_score,
            self.scorer_,
        )

        _logger.info(
            &#34;Searching the best hyperparameters using {} &#34;
            &#34;samples...&#34;.format(_num_samples(self.sample_indices_))
        )

        self.study_.optimize(
            objective, n_jobs=self.n_jobs, n_trials=self.n_trials, timeout=self.timeout
        )

        _logger.info(&#34;Finished hyperparemeter search!&#34;)

        if self.refit:
            self._refit(X, y, **fit_params)

        _logger.setLevel(old_level)

        return self

    def score(
        self,
        X: TwoDimArrayLikeType,
        y: Optional[Union[OneDimArrayLikeType, TwoDimArrayLikeType]] = None,
    ) -&gt; float:
        &#34;&#34;&#34;Return the score on the given data.

        Args:
            X:
                Data.

            y:
                Target variable.

        Returns:
            score:
                Scaler score.
        &#34;&#34;&#34;

        return self.scorer_(self.best_estimator_, X, y)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="optuna.integration.sklearn.OptunaSearchCV"><code class="flex name class">
<span>class <span class="ident">OptunaSearchCV</span></span>
<span>(</span><span>estimator: BaseEstimator, param_distributions: Mapping[str, <a title="optuna.distributions.BaseDistribution" href="../distributions.html#optuna.distributions.BaseDistribution">BaseDistribution</a>], cv: Union[_ForwardRef('BaseCrossValidator'), int, NoneType] = 5, enable_pruning: bool = False, error_score: Union[numbers.Number, str] = nan, max_iter: int = 1000, n_jobs: int = 1, n_trials: int = 10, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, refit: bool = True, return_train_score: bool = False, scoring: Union[Callable[..., float], str, NoneType] = None, study: Union[<a title="optuna.study.Study" href="../study.html#optuna.study.Study">Study</a>, NoneType] = None, subsample: Union[float, int] = 1.0, timeout: Union[float, NoneType] = None, verbose: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Hyperparameter search with cross-validation.</p>
<h2 id="args">Args</h2>
<p>estimator:
Object to use to fit the data. This is assumed to implement the
scikit-learn estimator interface. Either this needs to provide
<code>score</code>, or <code>scoring</code> must be passed.</p>
<p>param_distributions:
Dictionary where keys are parameters and values are distributions.
Distributions are assumed to implement the optuna distribution
interface.</p>
<p>cv:
Cross-validation strategy. Possible inputs for cv are:</p>
<pre><code>- integer to specify the number of folds in a CV splitter,
- a CV splitter,
- an iterable yielding (train, validation) splits as arrays of indices.

For integer, if :obj:&lt;code&gt;estimator&lt;/code&gt; is a classifier and :obj:&lt;code&gt;y&lt;/code&gt; is
either binary or multiclass,
&lt;code&gt;sklearn.model\_selection.StratifiedKFold&lt;/code&gt; is used. otherwise,
&lt;code&gt;sklearn.model\_selection.KFold&lt;/code&gt; is used.
</code></pre>
<p>enable_pruning:
If :obj:<code>True</code>, pruning is performed in the case where the
underlying estimator supports <code>partial_fit</code>.</p>
<p>error_score:
Value to assign to the score if an error occurs in fitting. If
'raise', the error is raised. If numeric,
<code>sklearn.exceptions.FitFailedWarning</code> is raised. This does not
affect the refit step, which will always raise the error.</p>
<p>max_iter:
Maximum number of epochs. This is only used if the underlying
estimator supports <code>partial_fit</code>.</p>
<p>n_jobs:
Number of parallel jobs. :obj:<code>-1</code> means using all processors.</p>
<p>n_trials:
Number of trials. If :obj:<code>None</code>, there is no limitation on the
number of trials. If :obj:<code>timeout</code> is also set to :obj:<code>None</code>,
the study continues to create trials until it receives a
termination signal such as Ctrl+C or SIGTERM. This trades off
runtime vs quality of the solution.</p>
<p>random_state:
Seed of the pseudo random number generator. If int, this is the
seed used by the random number generator. If
<code>numpy.random.RandomState</code> object, this is the random number
generator. If :obj:<code>None</code>, the global random state from
<code>numpy.random</code> is used.</p>
<p>refit:
If :obj:<code>True</code>, refit the estimator with the best found
hyperparameters. The refitted estimator is made available at the
<code>best_estimator_</code> attribute and permits using <code>predict</code>
directly.</p>
<p>return_train_score:
If :obj:<code>True</code>, training scores will be included. Computing
training scores is used to get insights on how different
hyperparameter settings impact the overfitting/underfitting
trade-off. However computing training scores can be
computationally expensive and is not strictly required to select
the hyperparameters that yield the best generalization
performance.</p>
<p>scoring:
String or callable to evaluate the predictions on the validation data.
If :obj:<code>None</code>, <code>score</code> on the estimator is used.</p>
<p>study:
Study corresponds to the optimization task. If :obj:<code>None</code>, a new
study is created.</p>
<p>subsample:
Proportion of samples that are used during hyperparameter search.</p>
<pre><code>- If int, then draw &lt;code&gt;subsample&lt;/code&gt; samples.
- If float, then draw &lt;code&gt;subsample&lt;/code&gt; * &lt;code&gt;X.shape\[0]&lt;/code&gt; samples.
</code></pre>
<p>timeout:
Time limit in seconds for the search of appropriate models. If
:obj:<code>None</code>, the study is executed without time limitation. If
:obj:<code>n_trials</code> is also set to :obj:<code>None</code>, the study continues to
create trials until it receives a termination signal such as
Ctrl+C or SIGTERM. This trades off runtime vs quality of the
solution.</p>
<p>verbose:
Verbosity level. The higher, the more messages.</p>
<h2 id="attributes">Attributes</h2>
<p>best_estimator_:
Estimator that was chosen by the search. This is present only if
<code>refit</code> is set to :obj:<code>True</code>.</p>
<p>n_splits_:
Number of cross-validation splits.</p>
<p>refit_time_:
Time for refitting the best estimator. This is present only if
<code>refit</code> is set to :obj:<code>True</code>.</p>
<p>sample_indices_:
Indices of samples that are used during hyperparameter search.</p>
<p>scorer_:
Scorer function.</p>
<p>study_:
Actual study.</p>
<h2 id="examples">Examples</h2>
<div class="admonition testcode">
<p class="admonition-title">Testcode</p>
<p>import optuna
from sklearn.datasets import load_iris
from sklearn.svm import SVC</p>
<p>clf = SVC(gamma='auto')
param_distributions = {
'C': optuna.distributions.LogUniformDistribution(1e-10, 1e+10)
}
optuna_search = optuna.integration.OptunaSearchCV(
clf,
param_distributions
)
X, y = load_iris(return_X_y=True)
optuna_search.fit(X, y)
y_pred = optuna_search.predict(X)</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Added in v0.17.0 as an experimental feature. The interface may change in newer versions
without prior notice. See <a href="https://github.com/optuna/optuna/releases/tag/v0.17.0.">https://github.com/optuna/optuna/releases/tag/v0.17.0.</a></p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OptunaSearchCV(BaseEstimator):
    &#34;&#34;&#34;Hyperparameter search with cross-validation.

    Args:
        estimator:
            Object to use to fit the data. This is assumed to implement the
            scikit-learn estimator interface. Either this needs to provide
            ``score``, or ``scoring`` must be passed.

        param_distributions:
            Dictionary where keys are parameters and values are distributions.
            Distributions are assumed to implement the optuna distribution
            interface.

        cv:
            Cross-validation strategy. Possible inputs for cv are:

            - integer to specify the number of folds in a CV splitter,
            - a CV splitter,
            - an iterable yielding (train, validation) splits as arrays of indices.

            For integer, if :obj:`estimator` is a classifier and :obj:`y` is
            either binary or multiclass,
            ``sklearn.model_selection.StratifiedKFold`` is used. otherwise,
            ``sklearn.model_selection.KFold`` is used.

        enable_pruning:
            If :obj:`True`, pruning is performed in the case where the
            underlying estimator supports ``partial_fit``.

        error_score:
            Value to assign to the score if an error occurs in fitting. If
            &#39;raise&#39;, the error is raised. If numeric,
            ``sklearn.exceptions.FitFailedWarning`` is raised. This does not
            affect the refit step, which will always raise the error.

        max_iter:
            Maximum number of epochs. This is only used if the underlying
            estimator supports ``partial_fit``.

        n_jobs:
            Number of parallel jobs. :obj:`-1` means using all processors.

        n_trials:
            Number of trials. If :obj:`None`, there is no limitation on the
            number of trials. If :obj:`timeout` is also set to :obj:`None`,
            the study continues to create trials until it receives a
            termination signal such as Ctrl+C or SIGTERM. This trades off
            runtime vs quality of the solution.

        random_state:
            Seed of the pseudo random number generator. If int, this is the
            seed used by the random number generator. If
            ``numpy.random.RandomState`` object, this is the random number
            generator. If :obj:`None`, the global random state from
            ``numpy.random`` is used.

        refit:
            If :obj:`True`, refit the estimator with the best found
            hyperparameters. The refitted estimator is made available at the
            ``best_estimator_`` attribute and permits using ``predict``
            directly.

        return_train_score:
            If :obj:`True`, training scores will be included. Computing
            training scores is used to get insights on how different
            hyperparameter settings impact the overfitting/underfitting
            trade-off. However computing training scores can be
            computationally expensive and is not strictly required to select
            the hyperparameters that yield the best generalization
            performance.

        scoring:
            String or callable to evaluate the predictions on the validation data.
            If :obj:`None`, ``score`` on the estimator is used.

        study:
            Study corresponds to the optimization task. If :obj:`None`, a new
            study is created.

        subsample:
            Proportion of samples that are used during hyperparameter search.

            - If int, then draw ``subsample`` samples.
            - If float, then draw ``subsample`` * ``X.shape[0]`` samples.

        timeout:
            Time limit in seconds for the search of appropriate models. If
            :obj:`None`, the study is executed without time limitation. If
            :obj:`n_trials` is also set to :obj:`None`, the study continues to
            create trials until it receives a termination signal such as
            Ctrl+C or SIGTERM. This trades off runtime vs quality of the
            solution.

        verbose:
            Verbosity level. The higher, the more messages.

    Attributes:
        best_estimator_:
            Estimator that was chosen by the search. This is present only if
            ``refit`` is set to :obj:`True`.

        n_splits_:
            Number of cross-validation splits.

        refit_time_:
            Time for refitting the best estimator. This is present only if
            ``refit`` is set to :obj:`True`.

        sample_indices_:
            Indices of samples that are used during hyperparameter search.

        scorer_:
            Scorer function.

        study_:
            Actual study.

    Examples:

        .. testcode::

                import optuna
                from sklearn.datasets import load_iris
                from sklearn.svm import SVC

                clf = SVC(gamma=&#39;auto&#39;)
                param_distributions = {
                    &#39;C&#39;: optuna.distributions.LogUniformDistribution(1e-10, 1e+10)
                }
                optuna_search = optuna.integration.OptunaSearchCV(
                    clf,
                    param_distributions
                )
                X, y = load_iris(return_X_y=True)
                optuna_search.fit(X, y)
                y_pred = optuna_search.predict(X)
    &#34;&#34;&#34;

    _required_parameters = [&#34;estimator&#34;, &#34;param_distributions&#34;]

    @property
    def _estimator_type(self) -&gt; str:

        return self.estimator._estimator_type

    @property
    def best_index_(self) -&gt; int:
        &#34;&#34;&#34;Index which corresponds to the best candidate parameter setting.&#34;&#34;&#34;

        df = self.trials_dataframe()

        return df[&#34;value&#34;].idxmin()

    @property
    def best_params_(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Parameters of the best trial in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.best_params

    @property
    def best_score_(self) -&gt; float:
        &#34;&#34;&#34;Mean cross-validated score of the best estimator.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.best_value

    @property
    def best_trial_(self) -&gt; FrozenTrial:
        &#34;&#34;&#34;Best trial in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.best_trial

    @property
    def classes_(self) -&gt; OneDimArrayLikeType:
        &#34;&#34;&#34;Class labels.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.classes_

    @property
    def n_trials_(self) -&gt; int:
        &#34;&#34;&#34;Actual number of trials.&#34;&#34;&#34;

        return len(self.trials_)

    @property
    def trials_(self) -&gt; List[FrozenTrial]:
        &#34;&#34;&#34;All trials in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.trials

    @property
    def user_attrs_(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;User attributes in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.user_attrs

    @property
    def decision_function(self) -&gt; Callable[..., Union[OneDimArrayLikeType, TwoDimArrayLikeType]]:
        &#34;&#34;&#34;Call ``decision_function`` on the best estimator.

        This is available only if the underlying estimator supports
        ``decision_function`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.decision_function

    @property
    def inverse_transform(self) -&gt; Callable[..., TwoDimArrayLikeType]:
        &#34;&#34;&#34;Call ``inverse_transform`` on the best estimator.

        This is available only if the underlying estimator supports
        ``inverse_transform`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.inverse_transform

    @property
    def predict(self) -&gt; Callable[..., Union[OneDimArrayLikeType, TwoDimArrayLikeType]]:
        &#34;&#34;&#34;Call ``predict`` on the best estimator.

        This is available only if the underlying estimator supports ``predict``
        and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.predict

    @property
    def predict_log_proba(self) -&gt; Callable[..., TwoDimArrayLikeType]:
        &#34;&#34;&#34;Call ``predict_log_proba`` on the best estimator.

        This is available only if the underlying estimator supports
        ``predict_log_proba`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.predict_log_proba

    @property
    def predict_proba(self) -&gt; Callable[..., TwoDimArrayLikeType]:
        &#34;&#34;&#34;Call ``predict_proba`` on the best estimator.

        This is available only if the underlying estimator supports
        ``predict_proba`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.predict_proba

    @property
    def score_samples(self) -&gt; Callable[..., OneDimArrayLikeType]:
        &#34;&#34;&#34;Call ``score_samples`` on the best estimator.

        This is available only if the underlying estimator supports
        ``score_samples`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.score_samples

    @property
    def set_user_attr(self) -&gt; Callable[..., None]:
        &#34;&#34;&#34;Call ``set_user_attr`` on the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.set_user_attr

    @property
    def transform(self) -&gt; Callable[..., TwoDimArrayLikeType]:
        &#34;&#34;&#34;Call ``transform`` on the best estimator.

        This is available only if the underlying estimator supports
        ``transform`` and ``refit`` is set to :obj:`True`.
        &#34;&#34;&#34;

        self._check_is_fitted()

        return self.best_estimator_.transform

    @property
    def trials_dataframe(self) -&gt; Callable[..., &#34;pd.DataFrame&#34;]:
        &#34;&#34;&#34;Call ``trials_dataframe`` on the :class:`~optuna.study.Study`.&#34;&#34;&#34;

        self._check_is_fitted()

        return self.study_.trials_dataframe

    def __init__(
        self,
        estimator: &#34;BaseEstimator&#34;,
        param_distributions: Mapping[str, distributions.BaseDistribution],
        cv: Optional[Union[&#34;BaseCrossValidator&#34;, int]] = 5,
        enable_pruning: bool = False,
        error_score: Union[Number, str] = np.nan,
        max_iter: int = 1000,
        n_jobs: int = 1,
        n_trials: int = 10,
        random_state: Optional[Union[int, np.random.RandomState]] = None,
        refit: bool = True,
        return_train_score: bool = False,
        scoring: Optional[Union[Callable[..., float], str]] = None,
        study: Optional[study_module.Study] = None,
        subsample: Union[float, int] = 1.0,
        timeout: Optional[float] = None,
        verbose: int = 0,
    ) -&gt; None:

        _imports.check()

        self.cv = cv
        self.enable_pruning = enable_pruning
        self.error_score = error_score
        self.estimator = estimator
        self.max_iter = max_iter
        self.n_trials = n_trials
        self.n_jobs = n_jobs
        self.param_distributions = param_distributions
        self.random_state = random_state
        self.refit = refit
        self.return_train_score = return_train_score
        self.scoring = scoring
        self.study = study
        self.subsample = subsample
        self.timeout = timeout
        self.verbose = verbose

    def _check_is_fitted(self) -&gt; None:

        attributes = [&#34;n_splits_&#34;, &#34;sample_indices_&#34;, &#34;scorer_&#34;, &#34;study_&#34;]

        if self.refit:
            attributes += [&#34;best_estimator_&#34;, &#34;refit_time_&#34;]

        check_is_fitted(self, attributes)

    def _check_params(self) -&gt; None:

        if not hasattr(self.estimator, &#34;fit&#34;):
            raise ValueError(&#34;estimator must be a scikit-learn estimator.&#34;)

        if type(self.param_distributions) is not dict:
            raise ValueError(&#34;param_distributions must be a dictionary.&#34;)

        for name, distribution in self.param_distributions.items():
            if not isinstance(distribution, distributions.BaseDistribution):
                raise ValueError(&#34;Value of {} must be a optuna distribution.&#34;.format(name))

        if self.enable_pruning and not hasattr(self.estimator, &#34;partial_fit&#34;):
            raise ValueError(&#34;estimator must support partial_fit.&#34;)

        if self.max_iter &lt;= 0:
            raise ValueError(&#34;max_iter must be &gt; 0, got {}.&#34;.format(self.max_iter))

        if self.study is not None and self.study.direction != StudyDirection.MAXIMIZE:
            raise ValueError(&#34;direction of study must be &#39;maximize&#39;.&#34;)

    def _more_tags(self) -&gt; Dict[str, bool]:

        return {&#34;non_deterministic&#34;: True, &#34;no_validation&#34;: True}

    def _refit(
        self,
        X: TwoDimArrayLikeType,
        y: Optional[Union[OneDimArrayLikeType, TwoDimArrayLikeType]] = None,
        **fit_params: Any
    ) -&gt; &#34;OptunaSearchCV&#34;:

        n_samples = _num_samples(X)

        self.best_estimator_ = clone(self.estimator)

        try:
            self.best_estimator_.set_params(**self.study_.best_params)
        except ValueError as e:
            _logger.exception(e)

        _logger.info(&#34;Refitting the estimator using {} samples...&#34;.format(n_samples))

        start_time = time()

        self.best_estimator_.fit(X, y, **fit_params)

        self.refit_time_ = time() - start_time

        _logger.info(&#34;Finished refitting! (elapsed time: {:.3f} sec.)&#34;.format(self.refit_time_))

        return self

    def fit(
        self,
        X: TwoDimArrayLikeType,
        y: Optional[Union[OneDimArrayLikeType, TwoDimArrayLikeType]] = None,
        groups: Optional[OneDimArrayLikeType] = None,
        **fit_params: Any
    ) -&gt; &#34;OptunaSearchCV&#34;:
        &#34;&#34;&#34;Run fit with all sets of parameters.

        Args:
            X:
                Training data.

            y:
                Target variable.

            groups:
                Group labels for the samples used while splitting the dataset
                into train/validation set.

            **fit_params:
                Parameters passed to ``fit`` on the estimator.

        Returns:
            self:
                Return self.
        &#34;&#34;&#34;

        self._check_params()

        random_state = check_random_state(self.random_state)
        max_samples = self.subsample
        n_samples = _num_samples(X)
        old_level = _logger.getEffectiveLevel()

        if self.verbose &gt; 1:
            _logger.setLevel(DEBUG)
        elif self.verbose &gt; 0:
            _logger.setLevel(INFO)
        else:
            _logger.setLevel(WARNING)

        self.sample_indices_ = np.arange(n_samples)

        if type(max_samples) is float:
            max_samples = int(max_samples * n_samples)

        if max_samples &lt; n_samples:
            self.sample_indices_ = random_state.choice(
                self.sample_indices_, max_samples, replace=False
            )

            self.sample_indices_.sort()

        X_res = _safe_indexing(X, self.sample_indices_)
        y_res = _safe_indexing(y, self.sample_indices_)
        groups_res = _safe_indexing(groups, self.sample_indices_)
        fit_params_res = fit_params

        if fit_params_res is not None:
            fit_params_res = _check_fit_params(X, fit_params, self.sample_indices_)

        classifier = is_classifier(self.estimator)
        cv = check_cv(self.cv, y_res, classifier)

        self.n_splits_ = cv.get_n_splits(X_res, y_res, groups=groups_res)
        self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)

        if self.study is None:
            seed = random_state.randint(0, np.iinfo(&#34;int32&#34;).max)
            sampler = samplers.TPESampler(seed=seed)

            self.study_ = study_module.create_study(direction=&#34;maximize&#34;, sampler=sampler)

        else:
            self.study_ = self.study

        objective = _Objective(
            self.estimator,
            self.param_distributions,
            X_res,
            y_res,
            cv,
            self.enable_pruning,
            self.error_score,
            fit_params_res,
            groups_res,
            self.max_iter,
            self.return_train_score,
            self.scorer_,
        )

        _logger.info(
            &#34;Searching the best hyperparameters using {} &#34;
            &#34;samples...&#34;.format(_num_samples(self.sample_indices_))
        )

        self.study_.optimize(
            objective, n_jobs=self.n_jobs, n_trials=self.n_trials, timeout=self.timeout
        )

        _logger.info(&#34;Finished hyperparemeter search!&#34;)

        if self.refit:
            self._refit(X, y, **fit_params)

        _logger.setLevel(old_level)

        return self

    def score(
        self,
        X: TwoDimArrayLikeType,
        y: Optional[Union[OneDimArrayLikeType, TwoDimArrayLikeType]] = None,
    ) -&gt; float:
        &#34;&#34;&#34;Return the score on the given data.

        Args:
            X:
                Data.

            y:
                Target variable.

        Returns:
            score:
                Scaler score.
        &#34;&#34;&#34;

        return self.scorer_(self.best_estimator_, X, y)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="optuna.integration.sklearn.OptunaSearchCV.best_index_"><code class="name">var <span class="ident">best_index_</span> : int</code></dt>
<dd>
<div class="desc"><p>Index which corresponds to the best candidate parameter setting.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def best_index_(self) -&gt; int:
    &#34;&#34;&#34;Index which corresponds to the best candidate parameter setting.&#34;&#34;&#34;

    df = self.trials_dataframe()

    return df[&#34;value&#34;].idxmin()</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.best_params_"><code class="name">var <span class="ident">best_params_</span> : Dict[str, Any]</code></dt>
<dd>
<div class="desc"><p>Parameters of the best trial in the :class:<code>~optuna.study.Study</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def best_params_(self) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;Parameters of the best trial in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

    self._check_is_fitted()

    return self.study_.best_params</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.best_score_"><code class="name">var <span class="ident">best_score_</span> : float</code></dt>
<dd>
<div class="desc"><p>Mean cross-validated score of the best estimator.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def best_score_(self) -&gt; float:
    &#34;&#34;&#34;Mean cross-validated score of the best estimator.&#34;&#34;&#34;

    self._check_is_fitted()

    return self.study_.best_value</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.best_trial_"><code class="name">var <span class="ident">best_trial_</span> : optuna.trial._frozen.FrozenTrial</code></dt>
<dd>
<div class="desc"><p>Best trial in the :class:<code>~optuna.study.Study</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def best_trial_(self) -&gt; FrozenTrial:
    &#34;&#34;&#34;Best trial in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

    self._check_is_fitted()

    return self.study_.best_trial</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.classes_"><code class="name">var <span class="ident">classes_</span> : Union[List[float], numpy.ndarray, pandas.core.series.Series]</code></dt>
<dd>
<div class="desc"><p>Class labels.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def classes_(self) -&gt; OneDimArrayLikeType:
    &#34;&#34;&#34;Class labels.&#34;&#34;&#34;

    self._check_is_fitted()

    return self.best_estimator_.classes_</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.decision_function"><code class="name">var <span class="ident">decision_function</span> : Callable[..., Union[List[float], numpy.ndarray, pandas.core.series.Series, List[List[float]], pandas.core.frame.DataFrame, scipy.sparse.base.spmatrix]]</code></dt>
<dd>
<div class="desc"><p>Call <code>decision_function</code> on the best estimator.</p>
<p>This is available only if the underlying estimator supports
<code>decision_function</code> and <code>refit</code> is set to :obj:<code>True</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def decision_function(self) -&gt; Callable[..., Union[OneDimArrayLikeType, TwoDimArrayLikeType]]:
    &#34;&#34;&#34;Call ``decision_function`` on the best estimator.

    This is available only if the underlying estimator supports
    ``decision_function`` and ``refit`` is set to :obj:`True`.
    &#34;&#34;&#34;

    self._check_is_fitted()

    return self.best_estimator_.decision_function</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.inverse_transform"><code class="name">var <span class="ident">inverse_transform</span> : Callable[..., Union[List[List[float]], numpy.ndarray, pandas.core.frame.DataFrame, scipy.sparse.base.spmatrix]]</code></dt>
<dd>
<div class="desc"><p>Call <code>inverse_transform</code> on the best estimator.</p>
<p>This is available only if the underlying estimator supports
<code>inverse_transform</code> and <code>refit</code> is set to :obj:<code>True</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def inverse_transform(self) -&gt; Callable[..., TwoDimArrayLikeType]:
    &#34;&#34;&#34;Call ``inverse_transform`` on the best estimator.

    This is available only if the underlying estimator supports
    ``inverse_transform`` and ``refit`` is set to :obj:`True`.
    &#34;&#34;&#34;

    self._check_is_fitted()

    return self.best_estimator_.inverse_transform</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.n_trials_"><code class="name">var <span class="ident">n_trials_</span> : int</code></dt>
<dd>
<div class="desc"><p>Actual number of trials.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def n_trials_(self) -&gt; int:
    &#34;&#34;&#34;Actual number of trials.&#34;&#34;&#34;

    return len(self.trials_)</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.predict"><code class="name">var <span class="ident">predict</span> : Callable[..., Union[List[float], numpy.ndarray, pandas.core.series.Series, List[List[float]], pandas.core.frame.DataFrame, scipy.sparse.base.spmatrix]]</code></dt>
<dd>
<div class="desc"><p>Call <code>predict</code> on the best estimator.</p>
<p>This is available only if the underlying estimator supports <code>predict</code>
and <code>refit</code> is set to :obj:<code>True</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def predict(self) -&gt; Callable[..., Union[OneDimArrayLikeType, TwoDimArrayLikeType]]:
    &#34;&#34;&#34;Call ``predict`` on the best estimator.

    This is available only if the underlying estimator supports ``predict``
    and ``refit`` is set to :obj:`True`.
    &#34;&#34;&#34;

    self._check_is_fitted()

    return self.best_estimator_.predict</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.predict_log_proba"><code class="name">var <span class="ident">predict_log_proba</span> : Callable[..., Union[List[List[float]], numpy.ndarray, pandas.core.frame.DataFrame, scipy.sparse.base.spmatrix]]</code></dt>
<dd>
<div class="desc"><p>Call <code>predict_log_proba</code> on the best estimator.</p>
<p>This is available only if the underlying estimator supports
<code>predict_log_proba</code> and <code>refit</code> is set to :obj:<code>True</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def predict_log_proba(self) -&gt; Callable[..., TwoDimArrayLikeType]:
    &#34;&#34;&#34;Call ``predict_log_proba`` on the best estimator.

    This is available only if the underlying estimator supports
    ``predict_log_proba`` and ``refit`` is set to :obj:`True`.
    &#34;&#34;&#34;

    self._check_is_fitted()

    return self.best_estimator_.predict_log_proba</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.predict_proba"><code class="name">var <span class="ident">predict_proba</span> : Callable[..., Union[List[List[float]], numpy.ndarray, pandas.core.frame.DataFrame, scipy.sparse.base.spmatrix]]</code></dt>
<dd>
<div class="desc"><p>Call <code>predict_proba</code> on the best estimator.</p>
<p>This is available only if the underlying estimator supports
<code>predict_proba</code> and <code>refit</code> is set to :obj:<code>True</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def predict_proba(self) -&gt; Callable[..., TwoDimArrayLikeType]:
    &#34;&#34;&#34;Call ``predict_proba`` on the best estimator.

    This is available only if the underlying estimator supports
    ``predict_proba`` and ``refit`` is set to :obj:`True`.
    &#34;&#34;&#34;

    self._check_is_fitted()

    return self.best_estimator_.predict_proba</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.score_samples"><code class="name">var <span class="ident">score_samples</span> : Callable[..., Union[List[float], numpy.ndarray, pandas.core.series.Series]]</code></dt>
<dd>
<div class="desc"><p>Call <code>score_samples</code> on the best estimator.</p>
<p>This is available only if the underlying estimator supports
<code>score_samples</code> and <code>refit</code> is set to :obj:<code>True</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def score_samples(self) -&gt; Callable[..., OneDimArrayLikeType]:
    &#34;&#34;&#34;Call ``score_samples`` on the best estimator.

    This is available only if the underlying estimator supports
    ``score_samples`` and ``refit`` is set to :obj:`True`.
    &#34;&#34;&#34;

    self._check_is_fitted()

    return self.best_estimator_.score_samples</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.set_user_attr"><code class="name">var <span class="ident">set_user_attr</span> : Callable[..., NoneType]</code></dt>
<dd>
<div class="desc"><p>Call <code>set_user_attr</code> on the :class:<code>~optuna.study.Study</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def set_user_attr(self) -&gt; Callable[..., None]:
    &#34;&#34;&#34;Call ``set_user_attr`` on the :class:`~optuna.study.Study`.&#34;&#34;&#34;

    self._check_is_fitted()

    return self.study_.set_user_attr</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.transform"><code class="name">var <span class="ident">transform</span> : Callable[..., Union[List[List[float]], numpy.ndarray, pandas.core.frame.DataFrame, scipy.sparse.base.spmatrix]]</code></dt>
<dd>
<div class="desc"><p>Call <code>transform</code> on the best estimator.</p>
<p>This is available only if the underlying estimator supports
<code>transform</code> and <code>refit</code> is set to :obj:<code>True</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def transform(self) -&gt; Callable[..., TwoDimArrayLikeType]:
    &#34;&#34;&#34;Call ``transform`` on the best estimator.

    This is available only if the underlying estimator supports
    ``transform`` and ``refit`` is set to :obj:`True`.
    &#34;&#34;&#34;

    self._check_is_fitted()

    return self.best_estimator_.transform</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.trials_"><code class="name">var <span class="ident">trials_</span> : List[optuna.trial._frozen.FrozenTrial]</code></dt>
<dd>
<div class="desc"><p>All trials in the :class:<code>~optuna.study.Study</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def trials_(self) -&gt; List[FrozenTrial]:
    &#34;&#34;&#34;All trials in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

    self._check_is_fitted()

    return self.study_.trials</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.trials_dataframe"><code class="name">var <span class="ident">trials_dataframe</span> : Callable[..., pandas.core.frame.DataFrame]</code></dt>
<dd>
<div class="desc"><p>Call <code>trials_dataframe</code> on the :class:<code>~optuna.study.Study</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def trials_dataframe(self) -&gt; Callable[..., &#34;pd.DataFrame&#34;]:
    &#34;&#34;&#34;Call ``trials_dataframe`` on the :class:`~optuna.study.Study`.&#34;&#34;&#34;

    self._check_is_fitted()

    return self.study_.trials_dataframe</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.user_attrs_"><code class="name">var <span class="ident">user_attrs_</span> : Dict[str, Any]</code></dt>
<dd>
<div class="desc"><p>User attributes in the :class:<code>~optuna.study.Study</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def user_attrs_(self) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;User attributes in the :class:`~optuna.study.Study`.&#34;&#34;&#34;

    self._check_is_fitted()

    return self.study_.user_attrs</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="optuna.integration.sklearn.OptunaSearchCV.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X: Union[List[List[float]], numpy.ndarray, _ForwardRef('pd.DataFrame'), scipy.sparse.base.spmatrix], y: Union[List[float], numpy.ndarray, _ForwardRef('pd.Series'), List[List[float]], _ForwardRef('pd.DataFrame'), scipy.sparse.base.spmatrix, NoneType] = None, groups: Union[List[float], numpy.ndarray, _ForwardRef('pd.Series'), NoneType] = None, **fit_params: Any) ‑> <a title="optuna.integration.sklearn.OptunaSearchCV" href="#optuna.integration.sklearn.OptunaSearchCV">OptunaSearchCV</a></span>
</code></dt>
<dd>
<div class="desc"><p>Run fit with all sets of parameters.</p>
<h2 id="args">Args</h2>
<p>X:
Training data.</p>
<p>y:
Target variable.</p>
<p>groups:
Group labels for the samples used while splitting the dataset
into train/validation set.</p>
<p>**fit_params:
Parameters passed to <code>fit</code> on the estimator.</p>
<h2 id="returns">Returns</h2>
<p>self:
Return self.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(
    self,
    X: TwoDimArrayLikeType,
    y: Optional[Union[OneDimArrayLikeType, TwoDimArrayLikeType]] = None,
    groups: Optional[OneDimArrayLikeType] = None,
    **fit_params: Any
) -&gt; &#34;OptunaSearchCV&#34;:
    &#34;&#34;&#34;Run fit with all sets of parameters.

    Args:
        X:
            Training data.

        y:
            Target variable.

        groups:
            Group labels for the samples used while splitting the dataset
            into train/validation set.

        **fit_params:
            Parameters passed to ``fit`` on the estimator.

    Returns:
        self:
            Return self.
    &#34;&#34;&#34;

    self._check_params()

    random_state = check_random_state(self.random_state)
    max_samples = self.subsample
    n_samples = _num_samples(X)
    old_level = _logger.getEffectiveLevel()

    if self.verbose &gt; 1:
        _logger.setLevel(DEBUG)
    elif self.verbose &gt; 0:
        _logger.setLevel(INFO)
    else:
        _logger.setLevel(WARNING)

    self.sample_indices_ = np.arange(n_samples)

    if type(max_samples) is float:
        max_samples = int(max_samples * n_samples)

    if max_samples &lt; n_samples:
        self.sample_indices_ = random_state.choice(
            self.sample_indices_, max_samples, replace=False
        )

        self.sample_indices_.sort()

    X_res = _safe_indexing(X, self.sample_indices_)
    y_res = _safe_indexing(y, self.sample_indices_)
    groups_res = _safe_indexing(groups, self.sample_indices_)
    fit_params_res = fit_params

    if fit_params_res is not None:
        fit_params_res = _check_fit_params(X, fit_params, self.sample_indices_)

    classifier = is_classifier(self.estimator)
    cv = check_cv(self.cv, y_res, classifier)

    self.n_splits_ = cv.get_n_splits(X_res, y_res, groups=groups_res)
    self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)

    if self.study is None:
        seed = random_state.randint(0, np.iinfo(&#34;int32&#34;).max)
        sampler = samplers.TPESampler(seed=seed)

        self.study_ = study_module.create_study(direction=&#34;maximize&#34;, sampler=sampler)

    else:
        self.study_ = self.study

    objective = _Objective(
        self.estimator,
        self.param_distributions,
        X_res,
        y_res,
        cv,
        self.enable_pruning,
        self.error_score,
        fit_params_res,
        groups_res,
        self.max_iter,
        self.return_train_score,
        self.scorer_,
    )

    _logger.info(
        &#34;Searching the best hyperparameters using {} &#34;
        &#34;samples...&#34;.format(_num_samples(self.sample_indices_))
    )

    self.study_.optimize(
        objective, n_jobs=self.n_jobs, n_trials=self.n_trials, timeout=self.timeout
    )

    _logger.info(&#34;Finished hyperparemeter search!&#34;)

    if self.refit:
        self._refit(X, y, **fit_params)

    _logger.setLevel(old_level)

    return self</code></pre>
</details>
</dd>
<dt id="optuna.integration.sklearn.OptunaSearchCV.score"><code class="name flex">
<span>def <span class="ident">score</span></span>(<span>self, X: Union[List[List[float]], numpy.ndarray, _ForwardRef('pd.DataFrame'), scipy.sparse.base.spmatrix], y: Union[List[float], numpy.ndarray, _ForwardRef('pd.Series'), List[List[float]], _ForwardRef('pd.DataFrame'), scipy.sparse.base.spmatrix, NoneType] = None) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Return the score on the given data.</p>
<h2 id="args">Args</h2>
<p>X:
Data.</p>
<p>y:
Target variable.</p>
<h2 id="returns">Returns</h2>
<p>score:
Scaler score.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score(
    self,
    X: TwoDimArrayLikeType,
    y: Optional[Union[OneDimArrayLikeType, TwoDimArrayLikeType]] = None,
) -&gt; float:
    &#34;&#34;&#34;Return the score on the given data.

    Args:
        X:
            Data.

        y:
            Target variable.

    Returns:
        score:
            Scaler score.
    &#34;&#34;&#34;

    return self.scorer_(self.best_estimator_, X, y)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="optuna.integration" href="index.html">optuna.integration</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="optuna.integration.sklearn.OptunaSearchCV" href="#optuna.integration.sklearn.OptunaSearchCV">OptunaSearchCV</a></code></h4>
<ul class="two-column">
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.best_index_" href="#optuna.integration.sklearn.OptunaSearchCV.best_index_">best_index_</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.best_params_" href="#optuna.integration.sklearn.OptunaSearchCV.best_params_">best_params_</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.best_score_" href="#optuna.integration.sklearn.OptunaSearchCV.best_score_">best_score_</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.best_trial_" href="#optuna.integration.sklearn.OptunaSearchCV.best_trial_">best_trial_</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.classes_" href="#optuna.integration.sklearn.OptunaSearchCV.classes_">classes_</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.decision_function" href="#optuna.integration.sklearn.OptunaSearchCV.decision_function">decision_function</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.fit" href="#optuna.integration.sklearn.OptunaSearchCV.fit">fit</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.inverse_transform" href="#optuna.integration.sklearn.OptunaSearchCV.inverse_transform">inverse_transform</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.n_trials_" href="#optuna.integration.sklearn.OptunaSearchCV.n_trials_">n_trials_</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.predict" href="#optuna.integration.sklearn.OptunaSearchCV.predict">predict</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.predict_log_proba" href="#optuna.integration.sklearn.OptunaSearchCV.predict_log_proba">predict_log_proba</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.predict_proba" href="#optuna.integration.sklearn.OptunaSearchCV.predict_proba">predict_proba</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.score" href="#optuna.integration.sklearn.OptunaSearchCV.score">score</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.score_samples" href="#optuna.integration.sklearn.OptunaSearchCV.score_samples">score_samples</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.set_user_attr" href="#optuna.integration.sklearn.OptunaSearchCV.set_user_attr">set_user_attr</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.transform" href="#optuna.integration.sklearn.OptunaSearchCV.transform">transform</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.trials_" href="#optuna.integration.sklearn.OptunaSearchCV.trials_">trials_</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.trials_dataframe" href="#optuna.integration.sklearn.OptunaSearchCV.trials_dataframe">trials_dataframe</a></code></li>
<li><code><a title="optuna.integration.sklearn.OptunaSearchCV.user_attrs_" href="#optuna.integration.sklearn.OptunaSearchCV.user_attrs_">user_attrs_</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>