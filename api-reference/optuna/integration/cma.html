<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>optuna.integration.cma API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>optuna.integration.cma</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import math
import random
from typing import Any
from typing import Dict
from typing import List
from typing import Optional
from typing import Set

import numpy

import optuna
from optuna._deprecated import deprecated
from optuna._imports import try_import
from optuna import distributions
from optuna.distributions import BaseDistribution
from optuna.distributions import CategoricalDistribution
from optuna.distributions import DiscreteUniformDistribution
from optuna.distributions import IntLogUniformDistribution
from optuna.distributions import IntUniformDistribution
from optuna.distributions import LogUniformDistribution
from optuna.distributions import UniformDistribution
from optuna import logging
from optuna.samplers import BaseSampler
from optuna.study import Study
from optuna.study import StudyDirection
from optuna.trial import FrozenTrial
from optuna.trial import TrialState

with try_import() as _imports:
    import cma

_logger = logging.get_logger(__name__)

_EPS = 1e-10


class PyCmaSampler(BaseSampler):
    &#34;&#34;&#34;A Sampler using cma library as the backend.

    Example:

        Optimize a simple quadratic function by using :class:`~optuna.integration.PyCmaSampler`.

        .. testcode::

            import optuna

            def objective(trial):
                x = trial.suggest_uniform(&#39;x&#39;, -1, 1)
                y = trial.suggest_int(&#39;y&#39;, -1, 1)
                return x**2 + y

            sampler = optuna.integration.PyCmaSampler()
            study = optuna.create_study(sampler=sampler)
            study.optimize(objective, n_trials=20)

    Note that parallel execution of trials may affect the optimization performance of CMA-ES,
    especially if the number of trials running in parallel exceeds the population size.

    .. note::
        :class:`~optuna.integration.CmaEsSampler` is deprecated and renamed to
        :class:`~optuna.integration.PyCmaSampler` in v2.0.0. Please use
        :class:`~optuna.integration.PyCmaSampler` instead of
        :class:`~optuna.integration.CmaEsSampler`.

    Args:

        x0:
            A dictionary of an initial parameter values for CMA-ES. By default, the mean of ``low``
            and ``high`` for each distribution is used.
            Please refer to cma.CMAEvolutionStrategy_ for further details of ``x0``.

        sigma0:
            Initial standard deviation of CMA-ES. By default, ``sigma0`` is set to
            ``min_range / 6``, where ``min_range`` denotes the minimum range of the distributions
            in the search space. If distribution is categorical, ``min_range`` is
            ``len(choices) - 1``.
            Please refer to cma.CMAEvolutionStrategy_ for further details of ``sigma0``.

        cma_stds:
            A dictionary of multipliers of sigma0 for each parameters. The default value is 1.0.
            Please refer to cma.CMAEvolutionStrategy_ for further details of ``cma_stds``.

        seed:
            A random seed for CMA-ES.

        cma_opts:
            Options passed to the constructor of cma.CMAEvolutionStrategy_ class.

            Note that ``BoundaryHandler``, ``bounds``, ``CMA_stds`` and ``seed`` arguments in
            ``cma_opts`` will be ignored because it is added by
            :class:`~optuna.integration.PyCmaSampler` automatically.

        n_startup_trials:
            The independent sampling is used instead of the CMA-ES algorithm until the given number
            of trials finish in the same study.

        independent_sampler:
            A :class:`~optuna.samplers.BaseSampler` instance that is used for independent
            sampling. The parameters not contained in the relative search space are sampled
            by this sampler.
            The search space for :class:`~optuna.integration.PyCmaSampler` is determined by
            :func:`~optuna.samplers.intersection_search_space()`.

            If :obj:`None` is specified, :class:`~optuna.samplers.RandomSampler` is used
            as the default.

            .. seealso::
                :class:`optuna.samplers` module provides built-in independent samplers
                such as :class:`~optuna.samplers.RandomSampler` and
                :class:`~optuna.samplers.TPESampler`.

        warn_independent_sampling:
            If this is :obj:`True`, a warning message is emitted when
            the value of a parameter is sampled by using an independent sampler.

            Note that the parameters of the first trial in a study are always sampled
            via an independent sampler, so no warning messages are emitted in this case.

    .. _cma.CMAEvolutionStrategy: http://cma.gforge.inria.fr/apidocs-pycma/\
    cma.evolution_strategy.CMAEvolutionStrategy.html
    &#34;&#34;&#34;

    def __init__(
        self,
        x0: Optional[Dict[str, Any]] = None,
        sigma0: Optional[float] = None,
        cma_stds: Optional[Dict[str, float]] = None,
        seed: Optional[int] = None,
        cma_opts: Optional[Dict[str, Any]] = None,
        n_startup_trials: int = 1,
        independent_sampler: Optional[BaseSampler] = None,
        warn_independent_sampling: bool = True,
    ) -&gt; None:

        _imports.check()

        self._x0 = x0
        self._sigma0 = sigma0
        self._cma_stds = cma_stds
        if seed is None:
            seed = random.randint(1, 2 ** 32)
        self._cma_opts = cma_opts or {}
        self._cma_opts[&#34;seed&#34;] = seed
        self._cma_opts.setdefault(&#34;verbose&#34;, -2)
        self._n_startup_trials = n_startup_trials
        self._independent_sampler = independent_sampler or optuna.samplers.RandomSampler(seed=seed)
        self._warn_independent_sampling = warn_independent_sampling
        self._logger = optuna.logging.get_logger(__name__)
        self._search_space = optuna.samplers.IntersectionSearchSpace()

    def reseed_rng(self) -&gt; None:

        self._cma_opts[&#34;seed&#34;] = random.randint(1, 2 ** 32)
        self._independent_sampler.reseed_rng()

    def infer_relative_search_space(
        self, study: Study, trial: FrozenTrial
    ) -&gt; Dict[str, BaseDistribution]:

        search_space = {}
        for name, distribution in self._search_space.calculate(study).items():
            if distribution.single():
                # `cma` cannot handle distributions that contain just a single value, so we skip
                # them. Note that the parameter values for such distributions are sampled in
                # `Trial`.
                continue

            search_space[name] = distribution

        return search_space

    def sample_independent(
        self,
        study: Study,
        trial: FrozenTrial,
        param_name: str,
        param_distribution: BaseDistribution,
    ) -&gt; float:

        if self._warn_independent_sampling:
            complete_trials = [t for t in study.trials if t.state == TrialState.COMPLETE]
            if len(complete_trials) &gt;= self._n_startup_trials:
                self._log_independent_sampling(trial, param_name)

        return self._independent_sampler.sample_independent(
            study, trial, param_name, param_distribution
        )

    def sample_relative(
        self, study: Study, trial: FrozenTrial, search_space: Dict[str, BaseDistribution]
    ) -&gt; Dict[str, float]:

        if len(search_space) == 0:
            return {}

        if len(search_space) == 1:
            self._logger.info(
                &#34;`PyCmaSampler` does not support optimization of 1-D search space. &#34;
                &#34;`{}` is used instead of `PyCmaSampler`.&#34;.format(
                    self._independent_sampler.__class__.__name__
                )
            )
            self._warn_independent_sampling = False
            return {}

        complete_trials = [t for t in study.trials if t.state == TrialState.COMPLETE]
        if len(complete_trials) &lt; self._n_startup_trials:
            return {}

        if self._x0 is None:
            self._x0 = self._initialize_x0(search_space)

        if self._sigma0 is None:
            sigma0 = self._initialize_sigma0(search_space)
        else:
            sigma0 = self._sigma0
        # Avoid ZeroDivisionError in cma.CMAEvolutionStrategy.
        sigma0 = max(sigma0, _EPS)

        optimizer = _Optimizer(search_space, self._x0, sigma0, self._cma_stds, self._cma_opts)
        trials = study.trials
        last_told_trial_number = optimizer.tell(trials, study.direction)
        return optimizer.ask(trials, last_told_trial_number)

    @staticmethod
    def _initialize_x0(search_space: Dict[str, BaseDistribution]) -&gt; Dict[str, Any]:

        x0 = {}
        for name, distribution in search_space.items():
            if isinstance(distribution, UniformDistribution):
                x0[name] = numpy.mean([distribution.high, distribution.low])
            elif isinstance(distribution, DiscreteUniformDistribution):
                x0[name] = numpy.mean([distribution.high, distribution.low])
            elif isinstance(distribution, IntUniformDistribution):
                x0[name] = int(numpy.mean([distribution.high, distribution.low]))
            elif isinstance(distribution, (LogUniformDistribution, IntLogUniformDistribution)):
                log_high = math.log(distribution.high)
                log_low = math.log(distribution.low)
                x0[name] = math.exp(numpy.mean([log_high, log_low]))
            elif isinstance(distribution, CategoricalDistribution):
                index = (len(distribution.choices) - 1) // 2
                x0[name] = distribution.choices[index]
            else:
                raise NotImplementedError(
                    &#34;The distribution {} is not implemented.&#34;.format(distribution)
                )
        return x0

    @staticmethod
    def _initialize_sigma0(search_space: Dict[str, BaseDistribution]) -&gt; float:

        sigma0s = []
        for name, distribution in search_space.items():
            if isinstance(distribution, UniformDistribution):
                sigma0s.append((distribution.high - distribution.low) / 6)
            elif isinstance(distribution, DiscreteUniformDistribution):
                sigma0s.append((distribution.high - distribution.low) / 6)
            elif isinstance(distribution, IntUniformDistribution):
                sigma0s.append((distribution.high - distribution.low) / 6)
            elif isinstance(distribution, (LogUniformDistribution, IntLogUniformDistribution)):
                log_high = math.log(distribution.high)
                log_low = math.log(distribution.low)
                sigma0s.append((log_high - log_low) / 6)
            elif isinstance(distribution, CategoricalDistribution):
                sigma0s.append((len(distribution.choices) - 1) / 6)
            else:
                raise NotImplementedError(
                    &#34;The distribution {} is not implemented.&#34;.format(distribution)
                )
        return min(sigma0s)

    def _log_independent_sampling(self, trial: FrozenTrial, param_name: str) -&gt; None:

        self._logger.warning(
            &#34;The parameter &#39;{}&#39; in trial#{} is sampled independently &#34;
            &#34;by using `{}` instead of `PyCmaSampler` &#34;
            &#34;(optimization performance may be degraded). &#34;
            &#34;You can suppress this warning by setting `warn_independent_sampling` &#34;
            &#34;to `False` in the constructor of `PyCmaSampler`, &#34;
            &#34;if this independent sampling is intended behavior.&#34;.format(
                param_name, trial.number, self._independent_sampler.__class__.__name__
            )
        )


class _Optimizer(object):
    def __init__(
        self,
        search_space: Dict[str, BaseDistribution],
        x0: Dict[str, Any],
        sigma0: float,
        cma_stds: Optional[Dict[str, float]],
        cma_opts: Dict[str, Any],
    ) -&gt; None:

        self._search_space = search_space
        self._param_names = list(sorted(self._search_space.keys()))

        lows = []
        highs = []
        for param_name in self._param_names:
            dist = self._search_space[param_name]
            if isinstance(dist, CategoricalDistribution):
                # Handle categorical values by ordinal representation.
                # TODO(Yanase): Support one-hot representation.
                lows.append(-0.5)
                highs.append(len(dist.choices) - 0.5)
            elif isinstance(dist, UniformDistribution) or isinstance(dist, LogUniformDistribution):
                lows.append(self._to_cma_params(search_space, param_name, dist.low))
                highs.append(self._to_cma_params(search_space, param_name, dist.high) - _EPS)
            elif isinstance(dist, DiscreteUniformDistribution):
                r = dist.high - dist.low
                lows.append(0 - 0.5 * dist.q)
                highs.append(r + 0.5 * dist.q)
            elif isinstance(dist, IntUniformDistribution):
                lows.append(dist.low - 0.5 * dist.step)
                highs.append(dist.high + 0.5 * dist.step)
            elif isinstance(dist, IntLogUniformDistribution):
                lows.append(self._to_cma_params(search_space, param_name, dist.low - 0.5))
                highs.append(self._to_cma_params(search_space, param_name, dist.high + 0.5))
            else:
                raise NotImplementedError(&#34;The distribution {} is not implemented.&#34;.format(dist))

        # Set initial params.
        initial_cma_params = []
        for param_name in self._param_names:
            initial_cma_params.append(
                self._to_cma_params(self._search_space, param_name, x0[param_name])
            )
        cma_option = {
            &#34;BoundaryHandler&#34;: cma.BoundTransform,
            &#34;bounds&#34;: [lows, highs],
        }

        if cma_stds:
            cma_option[&#34;CMA_stds&#34;] = [cma_stds.get(name, 1.0) for name in self._param_names]

        cma_opts.update(cma_option)

        self._es = cma.CMAEvolutionStrategy(initial_cma_params, sigma0, cma_opts)

    def tell(self, trials: List[FrozenTrial], study_direction: StudyDirection) -&gt; int:

        complete_trials = self._collect_target_trials(trials, target_states={TrialState.COMPLETE})

        popsize = self._es.popsize
        generation = len(complete_trials) // popsize
        last_told_trial_number = -1
        for i in range(generation):
            xs = []
            ys = []
            for t in complete_trials[i * popsize : (i + 1) * popsize]:
                x = [
                    self._to_cma_params(self._search_space, name, t.params[name])
                    for name in self._param_names
                ]
                xs.append(x)
                ys.append(t.value)
                last_told_trial_number = t.number
            if study_direction == StudyDirection.MAXIMIZE:
                ys = [-1 * y if y is not None else y for y in ys]

            # Calling `ask` is required to avoid RuntimeError which claims that `tell` should only
            # be called once per iteration.
            self._es.ask()
            self._es.tell(xs, ys)
        return last_told_trial_number

    def ask(self, trials: List[FrozenTrial], last_told_trial_number: int) -&gt; Dict[str, Any]:

        individual_index = len(self._collect_target_trials(trials, last_told_trial_number))
        popsize = self._es.popsize

        # individual_index may exceed the population size due to the parallel execution of multiple
        # trials. In such cases, `cma.cma.CMAEvolutionStrategy.ask` is called multiple times in an
        # iteration, and that may affect the optimization performance of CMA-ES.
        # In addition, please note that some trials may suggest the same parameters when multiple
        # samplers invoke this method simultaneously.
        while individual_index &gt;= popsize:
            individual_index -= popsize
            self._es.ask()
        cma_params = self._es.ask()[individual_index]

        ret_val = {}
        for param_name, value in zip(self._param_names, cma_params):
            ret_val[param_name] = self._to_optuna_params(self._search_space, param_name, value)
        return ret_val

    def _is_compatible(self, trial: FrozenTrial) -&gt; bool:

        # Thanks to `intersection_search_space()` function, in sequential optimization,
        # the parameters of complete trials are always compatible with the search space.
        #
        # However, in distributed optimization, incompatible trials may complete on a worker
        # just after an intersection search space is calculated on another worker.

        for name, distribution in self._search_space.items():
            if name not in trial.params:
                return False

            distributions.check_distribution_compatibility(distribution, trial.distributions[name])
            param_value = trial.params[name]
            param_internal_value = distribution.to_internal_repr(param_value)
            if not distribution._contains(param_internal_value):
                return False

        return True

    def _collect_target_trials(
        self,
        trials: List[FrozenTrial],
        last_told: int = -1,
        target_states: Optional[Set[TrialState]] = None,
    ) -&gt; List[FrozenTrial]:

        target_trials = [t for t in trials if t.number &gt; last_told]
        target_trials = [t for t in target_trials if self._is_compatible(t)]
        if target_states is not None:
            target_trials = [t for t in target_trials if t.state in target_states]

        return target_trials

    @staticmethod
    def _to_cma_params(
        search_space: Dict[str, BaseDistribution], param_name: str, optuna_param_value: Any
    ) -&gt; float:

        dist = search_space[param_name]
        if isinstance(dist, (LogUniformDistribution, IntLogUniformDistribution)):
            return math.log(optuna_param_value)
        elif isinstance(dist, DiscreteUniformDistribution):
            return optuna_param_value - dist.low
        elif isinstance(dist, CategoricalDistribution):
            return dist.choices.index(optuna_param_value)
        return optuna_param_value

    @staticmethod
    def _to_optuna_params(
        search_space: Dict[str, BaseDistribution], param_name: str, cma_param_value: float
    ) -&gt; Any:

        dist = search_space[param_name]
        if isinstance(dist, LogUniformDistribution):
            return math.exp(cma_param_value)
        if isinstance(dist, DiscreteUniformDistribution):
            v = numpy.round(cma_param_value / dist.q) * dist.q + dist.low
            # v may slightly exceed range due to round-off errors.
            return float(min(max(v, dist.low), dist.high))
        if isinstance(dist, IntUniformDistribution):
            r = numpy.round((cma_param_value - dist.low) / dist.step)
            v = r * dist.step + dist.low
            return int(v)
        if isinstance(dist, IntLogUniformDistribution):
            exp_value = math.exp(cma_param_value)
            v = numpy.round(exp_value)
            return int(min(max(v, dist.low), dist.high))

        if isinstance(dist, CategoricalDistribution):
            v = int(numpy.round(cma_param_value))
            return dist.choices[v]
        return cma_param_value


@deprecated(&#34;2.0.0&#34;, text=&#34;This class is renamed to :class:`~optuna.integration.PyCmaSampler`.&#34;)
class CmaEsSampler(PyCmaSampler):
    &#34;&#34;&#34;Wrapper class of PyCmaSampler for backward compatibility.&#34;&#34;&#34;

    def __init__(
        self,
        x0: Optional[Dict[str, Any]] = None,
        sigma0: Optional[float] = None,
        cma_stds: Optional[Dict[str, float]] = None,
        seed: Optional[int] = None,
        cma_opts: Optional[Dict[str, Any]] = None,
        n_startup_trials: int = 1,
        independent_sampler: Optional[BaseSampler] = None,
        warn_independent_sampling: bool = True,
    ) -&gt; None:

        super(CmaEsSampler, self).__init__(
            x0=x0,
            sigma0=sigma0,
            cma_stds=cma_stds,
            seed=seed,
            cma_opts=cma_opts,
            n_startup_trials=n_startup_trials,
            independent_sampler=independent_sampler,
            warn_independent_sampling=warn_independent_sampling,
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="optuna.integration.cma.CmaEsSampler"><code class="flex name class">
<span>class <span class="ident">CmaEsSampler</span></span>
<span>(</span><span>x0: Union[Dict[str, Any], NoneType] = None, sigma0: Union[float, NoneType] = None, cma_stds: Union[Dict[str, float], NoneType] = None, seed: Union[int, NoneType] = None, cma_opts: Union[Dict[str, Any], NoneType] = None, n_startup_trials: int = 1, independent_sampler: Union[optuna.samplers._base.BaseSampler, NoneType] = None, warn_independent_sampling: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper class of PyCmaSampler for backward compatibility.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Deprecated in v2.0.0. This feature will be removed in the future. The removal of this
feature is currently scheduled for v4.0.0, but this schedule is subject to change.
See <a href="https://github.com/optuna/optuna/releases/tag/v2.0.0.">https://github.com/optuna/optuna/releases/tag/v2.0.0.</a></p>
<p>This class is renamed to :class:<code>~optuna.integration.PyCmaSampler</code>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CmaEsSampler(PyCmaSampler):
    &#34;&#34;&#34;Wrapper class of PyCmaSampler for backward compatibility.&#34;&#34;&#34;

    def __init__(
        self,
        x0: Optional[Dict[str, Any]] = None,
        sigma0: Optional[float] = None,
        cma_stds: Optional[Dict[str, float]] = None,
        seed: Optional[int] = None,
        cma_opts: Optional[Dict[str, Any]] = None,
        n_startup_trials: int = 1,
        independent_sampler: Optional[BaseSampler] = None,
        warn_independent_sampling: bool = True,
    ) -&gt; None:

        super(CmaEsSampler, self).__init__(
            x0=x0,
            sigma0=sigma0,
            cma_stds=cma_stds,
            seed=seed,
            cma_opts=cma_opts,
            n_startup_trials=n_startup_trials,
            independent_sampler=independent_sampler,
            warn_independent_sampling=warn_independent_sampling,
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="optuna.integration.cma.PyCmaSampler" href="#optuna.integration.cma.PyCmaSampler">PyCmaSampler</a></li>
<li>optuna.samplers._base.BaseSampler</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="optuna.integration.cma.PyCmaSampler" href="#optuna.integration.cma.PyCmaSampler">PyCmaSampler</a></b></code>:
<ul class="hlist">
<li><code><a title="optuna.integration.cma.PyCmaSampler.infer_relative_search_space" href="#optuna.integration.cma.PyCmaSampler.infer_relative_search_space">infer_relative_search_space</a></code></li>
<li><code><a title="optuna.integration.cma.PyCmaSampler.reseed_rng" href="#optuna.integration.cma.PyCmaSampler.reseed_rng">reseed_rng</a></code></li>
<li><code><a title="optuna.integration.cma.PyCmaSampler.sample_independent" href="#optuna.integration.cma.PyCmaSampler.sample_independent">sample_independent</a></code></li>
<li><code><a title="optuna.integration.cma.PyCmaSampler.sample_relative" href="#optuna.integration.cma.PyCmaSampler.sample_relative">sample_relative</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="optuna.integration.cma.PyCmaSampler"><code class="flex name class">
<span>class <span class="ident">PyCmaSampler</span></span>
<span>(</span><span>x0: Union[Dict[str, Any], NoneType] = None, sigma0: Union[float, NoneType] = None, cma_stds: Union[Dict[str, float], NoneType] = None, seed: Union[int, NoneType] = None, cma_opts: Union[Dict[str, Any], NoneType] = None, n_startup_trials: int = 1, independent_sampler: Union[optuna.samplers._base.BaseSampler, NoneType] = None, warn_independent_sampling: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>A Sampler using cma library as the backend.</p>
<h2 id="example">Example</h2>
<p>Optimize a simple quadratic function by using :class:<code>~optuna.integration.PyCmaSampler</code>.</p>
<div class="admonition testcode">
<p class="admonition-title">Testcode</p>
<p>import optuna</p>
<p>def objective(trial):
x = trial.suggest_uniform('x', -1, 1)
y = trial.suggest_int('y', -1, 1)
return x**2 + y</p>
<p>sampler = optuna.integration.PyCmaSampler()
study = optuna.create_study(sampler=sampler)
study.optimize(objective, n_trials=20)</p>
</div>
<p>Note that parallel execution of trials may affect the optimization performance of CMA-ES,
especially if the number of trials running in parallel exceeds the population size.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>:class:<code>~optuna.integration.CmaEsSampler</code> is deprecated and renamed to
:class:<code>~optuna.integration.PyCmaSampler</code> in v2.0.0. Please use
:class:<code>~optuna.integration.PyCmaSampler</code> instead of
:class:<code>~optuna.integration.CmaEsSampler</code>.</p>
</div>
<h2 id="args">Args</h2>
<p>x0:
A dictionary of an initial parameter values for CMA-ES. By default, the mean of <code>low</code>
and <code>high</code> for each distribution is used.
Please refer to cma.CMAEvolutionStrategy_ for further details of <code>x0</code>.</p>
<p>sigma0:
Initial standard deviation of CMA-ES. By default, <code>sigma0</code> is set to
<code>min_range / 6</code>, where <code>min_range</code> denotes the minimum range of the distributions
in the search space. If distribution is categorical, <code>min_range</code> is
<code>len(choices) - 1</code>.
Please refer to cma.CMAEvolutionStrategy_ for further details of <code>sigma0</code>.</p>
<p>cma_stds:
A dictionary of multipliers of sigma0 for each parameters. The default value is 1.0.
Please refer to cma.CMAEvolutionStrategy_ for further details of <code>cma_stds</code>.</p>
<p>seed:
A random seed for CMA-ES.</p>
<p>cma_opts:
Options passed to the constructor of cma.CMAEvolutionStrategy_ class.</p>
<pre><code>Note that &lt;code&gt;BoundaryHandler&lt;/code&gt;, &lt;code&gt;bounds&lt;/code&gt;, &lt;code&gt;CMA\_stds&lt;/code&gt; and &lt;code&gt;seed&lt;/code&gt; arguments in
&lt;code&gt;cma\_opts&lt;/code&gt; will be ignored because it is added by
:class:`~optuna.integration.PyCmaSampler` automatically.
</code></pre>
<p>n_startup_trials:
The independent sampling is used instead of the CMA-ES algorithm until the given number
of trials finish in the same study.</p>
<p>independent_sampler:
A :class:<code>~optuna.samplers.BaseSampler</code> instance that is used for independent
sampling. The parameters not contained in the relative search space are sampled
by this sampler.
The search space for :class:<code>~optuna.integration.PyCmaSampler</code> is determined by
:func:<code>~optuna.samplers.intersection_search_space()</code>.</p>
<pre><code>If :obj:&lt;code&gt;None&lt;/code&gt; is specified, :class:`~optuna.samplers.RandomSampler` is used
as the default.

!!! seealso "Seealso"
    :class:&lt;code&gt;&lt;a title="optuna.samplers" href="../samplers/index.html"&gt;optuna.samplers&lt;/a&gt;&lt;/code&gt; module provides built-in independent samplers
    such as :class:`~optuna.samplers.RandomSampler` and
    :class:`~optuna.samplers.TPESampler`.
</code></pre>
<p>warn_independent_sampling:
If this is :obj:<code>True</code>, a warning message is emitted when
the value of a parameter is sampled by using an independent sampler.</p>
<pre><code>Note that the parameters of the first trial in a study are always sampled
via an independent sampler, so no warning messages are emitted in this case.
</code></pre>
<p>.. _cma.CMAEvolutionStrategy: <a href="http://cma.gforge.inria.fr/apidocs-pycma/">http://cma.gforge.inria.fr/apidocs-pycma/</a>
cma.evolution_strategy.CMAEvolutionStrategy.html</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PyCmaSampler(BaseSampler):
    &#34;&#34;&#34;A Sampler using cma library as the backend.

    Example:

        Optimize a simple quadratic function by using :class:`~optuna.integration.PyCmaSampler`.

        .. testcode::

            import optuna

            def objective(trial):
                x = trial.suggest_uniform(&#39;x&#39;, -1, 1)
                y = trial.suggest_int(&#39;y&#39;, -1, 1)
                return x**2 + y

            sampler = optuna.integration.PyCmaSampler()
            study = optuna.create_study(sampler=sampler)
            study.optimize(objective, n_trials=20)

    Note that parallel execution of trials may affect the optimization performance of CMA-ES,
    especially if the number of trials running in parallel exceeds the population size.

    .. note::
        :class:`~optuna.integration.CmaEsSampler` is deprecated and renamed to
        :class:`~optuna.integration.PyCmaSampler` in v2.0.0. Please use
        :class:`~optuna.integration.PyCmaSampler` instead of
        :class:`~optuna.integration.CmaEsSampler`.

    Args:

        x0:
            A dictionary of an initial parameter values for CMA-ES. By default, the mean of ``low``
            and ``high`` for each distribution is used.
            Please refer to cma.CMAEvolutionStrategy_ for further details of ``x0``.

        sigma0:
            Initial standard deviation of CMA-ES. By default, ``sigma0`` is set to
            ``min_range / 6``, where ``min_range`` denotes the minimum range of the distributions
            in the search space. If distribution is categorical, ``min_range`` is
            ``len(choices) - 1``.
            Please refer to cma.CMAEvolutionStrategy_ for further details of ``sigma0``.

        cma_stds:
            A dictionary of multipliers of sigma0 for each parameters. The default value is 1.0.
            Please refer to cma.CMAEvolutionStrategy_ for further details of ``cma_stds``.

        seed:
            A random seed for CMA-ES.

        cma_opts:
            Options passed to the constructor of cma.CMAEvolutionStrategy_ class.

            Note that ``BoundaryHandler``, ``bounds``, ``CMA_stds`` and ``seed`` arguments in
            ``cma_opts`` will be ignored because it is added by
            :class:`~optuna.integration.PyCmaSampler` automatically.

        n_startup_trials:
            The independent sampling is used instead of the CMA-ES algorithm until the given number
            of trials finish in the same study.

        independent_sampler:
            A :class:`~optuna.samplers.BaseSampler` instance that is used for independent
            sampling. The parameters not contained in the relative search space are sampled
            by this sampler.
            The search space for :class:`~optuna.integration.PyCmaSampler` is determined by
            :func:`~optuna.samplers.intersection_search_space()`.

            If :obj:`None` is specified, :class:`~optuna.samplers.RandomSampler` is used
            as the default.

            .. seealso::
                :class:`optuna.samplers` module provides built-in independent samplers
                such as :class:`~optuna.samplers.RandomSampler` and
                :class:`~optuna.samplers.TPESampler`.

        warn_independent_sampling:
            If this is :obj:`True`, a warning message is emitted when
            the value of a parameter is sampled by using an independent sampler.

            Note that the parameters of the first trial in a study are always sampled
            via an independent sampler, so no warning messages are emitted in this case.

    .. _cma.CMAEvolutionStrategy: http://cma.gforge.inria.fr/apidocs-pycma/\
    cma.evolution_strategy.CMAEvolutionStrategy.html
    &#34;&#34;&#34;

    def __init__(
        self,
        x0: Optional[Dict[str, Any]] = None,
        sigma0: Optional[float] = None,
        cma_stds: Optional[Dict[str, float]] = None,
        seed: Optional[int] = None,
        cma_opts: Optional[Dict[str, Any]] = None,
        n_startup_trials: int = 1,
        independent_sampler: Optional[BaseSampler] = None,
        warn_independent_sampling: bool = True,
    ) -&gt; None:

        _imports.check()

        self._x0 = x0
        self._sigma0 = sigma0
        self._cma_stds = cma_stds
        if seed is None:
            seed = random.randint(1, 2 ** 32)
        self._cma_opts = cma_opts or {}
        self._cma_opts[&#34;seed&#34;] = seed
        self._cma_opts.setdefault(&#34;verbose&#34;, -2)
        self._n_startup_trials = n_startup_trials
        self._independent_sampler = independent_sampler or optuna.samplers.RandomSampler(seed=seed)
        self._warn_independent_sampling = warn_independent_sampling
        self._logger = optuna.logging.get_logger(__name__)
        self._search_space = optuna.samplers.IntersectionSearchSpace()

    def reseed_rng(self) -&gt; None:

        self._cma_opts[&#34;seed&#34;] = random.randint(1, 2 ** 32)
        self._independent_sampler.reseed_rng()

    def infer_relative_search_space(
        self, study: Study, trial: FrozenTrial
    ) -&gt; Dict[str, BaseDistribution]:

        search_space = {}
        for name, distribution in self._search_space.calculate(study).items():
            if distribution.single():
                # `cma` cannot handle distributions that contain just a single value, so we skip
                # them. Note that the parameter values for such distributions are sampled in
                # `Trial`.
                continue

            search_space[name] = distribution

        return search_space

    def sample_independent(
        self,
        study: Study,
        trial: FrozenTrial,
        param_name: str,
        param_distribution: BaseDistribution,
    ) -&gt; float:

        if self._warn_independent_sampling:
            complete_trials = [t for t in study.trials if t.state == TrialState.COMPLETE]
            if len(complete_trials) &gt;= self._n_startup_trials:
                self._log_independent_sampling(trial, param_name)

        return self._independent_sampler.sample_independent(
            study, trial, param_name, param_distribution
        )

    def sample_relative(
        self, study: Study, trial: FrozenTrial, search_space: Dict[str, BaseDistribution]
    ) -&gt; Dict[str, float]:

        if len(search_space) == 0:
            return {}

        if len(search_space) == 1:
            self._logger.info(
                &#34;`PyCmaSampler` does not support optimization of 1-D search space. &#34;
                &#34;`{}` is used instead of `PyCmaSampler`.&#34;.format(
                    self._independent_sampler.__class__.__name__
                )
            )
            self._warn_independent_sampling = False
            return {}

        complete_trials = [t for t in study.trials if t.state == TrialState.COMPLETE]
        if len(complete_trials) &lt; self._n_startup_trials:
            return {}

        if self._x0 is None:
            self._x0 = self._initialize_x0(search_space)

        if self._sigma0 is None:
            sigma0 = self._initialize_sigma0(search_space)
        else:
            sigma0 = self._sigma0
        # Avoid ZeroDivisionError in cma.CMAEvolutionStrategy.
        sigma0 = max(sigma0, _EPS)

        optimizer = _Optimizer(search_space, self._x0, sigma0, self._cma_stds, self._cma_opts)
        trials = study.trials
        last_told_trial_number = optimizer.tell(trials, study.direction)
        return optimizer.ask(trials, last_told_trial_number)

    @staticmethod
    def _initialize_x0(search_space: Dict[str, BaseDistribution]) -&gt; Dict[str, Any]:

        x0 = {}
        for name, distribution in search_space.items():
            if isinstance(distribution, UniformDistribution):
                x0[name] = numpy.mean([distribution.high, distribution.low])
            elif isinstance(distribution, DiscreteUniformDistribution):
                x0[name] = numpy.mean([distribution.high, distribution.low])
            elif isinstance(distribution, IntUniformDistribution):
                x0[name] = int(numpy.mean([distribution.high, distribution.low]))
            elif isinstance(distribution, (LogUniformDistribution, IntLogUniformDistribution)):
                log_high = math.log(distribution.high)
                log_low = math.log(distribution.low)
                x0[name] = math.exp(numpy.mean([log_high, log_low]))
            elif isinstance(distribution, CategoricalDistribution):
                index = (len(distribution.choices) - 1) // 2
                x0[name] = distribution.choices[index]
            else:
                raise NotImplementedError(
                    &#34;The distribution {} is not implemented.&#34;.format(distribution)
                )
        return x0

    @staticmethod
    def _initialize_sigma0(search_space: Dict[str, BaseDistribution]) -&gt; float:

        sigma0s = []
        for name, distribution in search_space.items():
            if isinstance(distribution, UniformDistribution):
                sigma0s.append((distribution.high - distribution.low) / 6)
            elif isinstance(distribution, DiscreteUniformDistribution):
                sigma0s.append((distribution.high - distribution.low) / 6)
            elif isinstance(distribution, IntUniformDistribution):
                sigma0s.append((distribution.high - distribution.low) / 6)
            elif isinstance(distribution, (LogUniformDistribution, IntLogUniformDistribution)):
                log_high = math.log(distribution.high)
                log_low = math.log(distribution.low)
                sigma0s.append((log_high - log_low) / 6)
            elif isinstance(distribution, CategoricalDistribution):
                sigma0s.append((len(distribution.choices) - 1) / 6)
            else:
                raise NotImplementedError(
                    &#34;The distribution {} is not implemented.&#34;.format(distribution)
                )
        return min(sigma0s)

    def _log_independent_sampling(self, trial: FrozenTrial, param_name: str) -&gt; None:

        self._logger.warning(
            &#34;The parameter &#39;{}&#39; in trial#{} is sampled independently &#34;
            &#34;by using `{}` instead of `PyCmaSampler` &#34;
            &#34;(optimization performance may be degraded). &#34;
            &#34;You can suppress this warning by setting `warn_independent_sampling` &#34;
            &#34;to `False` in the constructor of `PyCmaSampler`, &#34;
            &#34;if this independent sampling is intended behavior.&#34;.format(
                param_name, trial.number, self._independent_sampler.__class__.__name__
            )
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>optuna.samplers._base.BaseSampler</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="optuna.integration.cma.CmaEsSampler" href="#optuna.integration.cma.CmaEsSampler">CmaEsSampler</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="optuna.integration.cma.PyCmaSampler.infer_relative_search_space"><code class="name flex">
<span>def <span class="ident">infer_relative_search_space</span></span>(<span>self, study: <a title="optuna.study.Study" href="../study.html#optuna.study.Study">Study</a>, trial: optuna.trial._frozen.FrozenTrial) ‑> Dict[str, <a title="optuna.distributions.BaseDistribution" href="../distributions.html#optuna.distributions.BaseDistribution">BaseDistribution</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Infer the search space that will be used by relative sampling in the target trial.</p>
<p>This method is called right before :func:<code>~optuna.samplers.BaseSampler.sample_relative</code>
method, and the search space returned by this method is pass to it. The parameters not
contained in the search space will be sampled by using
:func:<code>~optuna.samplers.BaseSampler.sample_independent</code> method.</p>
<h2 id="args">Args</h2>
<p>study:
Target study object.
trial:
Target trial object.
Take a copy before modifying this object.</p>
<div class="admonition seealso">
<p class="admonition-title">Seealso</p>
<p>Please refer to :func:<code>~optuna.samplers.intersection_search_space</code> as an
implementation of :func:<code>~optuna.samplers.BaseSampler.infer_relative_search_space</code>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def infer_relative_search_space(
    self, study: Study, trial: FrozenTrial
) -&gt; Dict[str, BaseDistribution]:

    search_space = {}
    for name, distribution in self._search_space.calculate(study).items():
        if distribution.single():
            # `cma` cannot handle distributions that contain just a single value, so we skip
            # them. Note that the parameter values for such distributions are sampled in
            # `Trial`.
            continue

        search_space[name] = distribution

    return search_space</code></pre>
</details>
</dd>
<dt id="optuna.integration.cma.PyCmaSampler.reseed_rng"><code class="name flex">
<span>def <span class="ident">reseed_rng</span></span>(<span>self) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Reseed sampler's random number generator.</p>
<p>This method is called by the :class:<code>~optuna.study.Study</code> instance if trials are executed
in parallel with the option <code>n_jobs&gt;1</code>. In that case, the sampler instance will be
replicated including the state of the random number generator, and they may suggest the
same values. To prevent this issue, this method assigns a different seed to each random
number generator.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reseed_rng(self) -&gt; None:

    self._cma_opts[&#34;seed&#34;] = random.randint(1, 2 ** 32)
    self._independent_sampler.reseed_rng()</code></pre>
</details>
</dd>
<dt id="optuna.integration.cma.PyCmaSampler.sample_independent"><code class="name flex">
<span>def <span class="ident">sample_independent</span></span>(<span>self, study: <a title="optuna.study.Study" href="../study.html#optuna.study.Study">Study</a>, trial: optuna.trial._frozen.FrozenTrial, param_name: str, param_distribution: <a title="optuna.distributions.BaseDistribution" href="../distributions.html#optuna.distributions.BaseDistribution">BaseDistribution</a>) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Sample a parameter for a given distribution.</p>
<p>This method is called only for the parameters not contained in the search space returned
by :func:<code>~optuna.samplers.BaseSampler.sample_relative</code> method. This method is suitable
for sampling algorithms that do not use relationship between parameters such as random
sampling and TPE.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The failed trials are ignored by any build-in samplers when they sample new
parameters. Thus, failed trials are regarded as deleted in the samplers'
perspective.</p>
</div>
<h2 id="args">Args</h2>
<p>study:
Target study object.
trial:
Target trial object.
Take a copy before modifying this object.
param_name:
Name of the sampled parameter.
param_distribution:
Distribution object that specifies a prior and/or scale of the sampling algorithm.</p>
<h2 id="returns">Returns</h2>
<p>A parameter value.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_independent(
    self,
    study: Study,
    trial: FrozenTrial,
    param_name: str,
    param_distribution: BaseDistribution,
) -&gt; float:

    if self._warn_independent_sampling:
        complete_trials = [t for t in study.trials if t.state == TrialState.COMPLETE]
        if len(complete_trials) &gt;= self._n_startup_trials:
            self._log_independent_sampling(trial, param_name)

    return self._independent_sampler.sample_independent(
        study, trial, param_name, param_distribution
    )</code></pre>
</details>
</dd>
<dt id="optuna.integration.cma.PyCmaSampler.sample_relative"><code class="name flex">
<span>def <span class="ident">sample_relative</span></span>(<span>self, study: <a title="optuna.study.Study" href="../study.html#optuna.study.Study">Study</a>, trial: optuna.trial._frozen.FrozenTrial, search_space: Dict[str, <a title="optuna.distributions.BaseDistribution" href="../distributions.html#optuna.distributions.BaseDistribution">BaseDistribution</a>]) ‑> Dict[str, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Sample parameters in a given search space.</p>
<p>This method is called once at the beginning of each trial, i.e., right before the
evaluation of the objective function. This method is suitable for sampling algorithms
that use relationship between parameters such as Gaussian Process and CMA-ES.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The failed trials are ignored by any build-in samplers when they sample new
parameters. Thus, failed trials are regarded as deleted in the samplers'
perspective.</p>
</div>
<h2 id="args">Args</h2>
<p>study:
Target study object.
trial:
Target trial object.
Take a copy before modifying this object.
search_space:
The search space returned by
:func:<code>~optuna.samplers.BaseSampler.infer_relative_search_space</code>.</p>
<h2 id="returns">Returns</h2>
<p>A dictionary containing the parameter names and the values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_relative(
    self, study: Study, trial: FrozenTrial, search_space: Dict[str, BaseDistribution]
) -&gt; Dict[str, float]:

    if len(search_space) == 0:
        return {}

    if len(search_space) == 1:
        self._logger.info(
            &#34;`PyCmaSampler` does not support optimization of 1-D search space. &#34;
            &#34;`{}` is used instead of `PyCmaSampler`.&#34;.format(
                self._independent_sampler.__class__.__name__
            )
        )
        self._warn_independent_sampling = False
        return {}

    complete_trials = [t for t in study.trials if t.state == TrialState.COMPLETE]
    if len(complete_trials) &lt; self._n_startup_trials:
        return {}

    if self._x0 is None:
        self._x0 = self._initialize_x0(search_space)

    if self._sigma0 is None:
        sigma0 = self._initialize_sigma0(search_space)
    else:
        sigma0 = self._sigma0
    # Avoid ZeroDivisionError in cma.CMAEvolutionStrategy.
    sigma0 = max(sigma0, _EPS)

    optimizer = _Optimizer(search_space, self._x0, sigma0, self._cma_stds, self._cma_opts)
    trials = study.trials
    last_told_trial_number = optimizer.tell(trials, study.direction)
    return optimizer.ask(trials, last_told_trial_number)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="optuna.integration" href="index.html">optuna.integration</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="optuna.integration.cma.CmaEsSampler" href="#optuna.integration.cma.CmaEsSampler">CmaEsSampler</a></code></h4>
</li>
<li>
<h4><code><a title="optuna.integration.cma.PyCmaSampler" href="#optuna.integration.cma.PyCmaSampler">PyCmaSampler</a></code></h4>
<ul class="">
<li><code><a title="optuna.integration.cma.PyCmaSampler.infer_relative_search_space" href="#optuna.integration.cma.PyCmaSampler.infer_relative_search_space">infer_relative_search_space</a></code></li>
<li><code><a title="optuna.integration.cma.PyCmaSampler.reseed_rng" href="#optuna.integration.cma.PyCmaSampler.reseed_rng">reseed_rng</a></code></li>
<li><code><a title="optuna.integration.cma.PyCmaSampler.sample_independent" href="#optuna.integration.cma.PyCmaSampler.sample_independent">sample_independent</a></code></li>
<li><code><a title="optuna.integration.cma.PyCmaSampler.sample_relative" href="#optuna.integration.cma.PyCmaSampler.sample_relative">sample_relative</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>